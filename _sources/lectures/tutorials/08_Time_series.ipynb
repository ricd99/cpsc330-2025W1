{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](../img/330-banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 8\n",
    "\n",
    "UBC 2024-26\n",
    "\n",
    "## Outline\n",
    "\n",
    "During this tutorial, you will \n",
    "\n",
    "All questions can be discussed with your classmates and the TAs - this is not a graded exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import (\n",
    "    TimeSeriesSplit,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"..\"), \"data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time series analysis on a more complicated dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the [rain in Australia](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package) dataset. Our goal is to predict whether or not it will rain tomorrow based on today's measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(DATA_DIR + \"weatherAUS.csv\")\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Questions of interest**\n",
    "\n",
    "- Can we **forecast** into the future? Can we predict whether it's going to rain tomorrow?\n",
    "    - The target variable is `RainTomorrow`. The target is categorical and not continuous in this case. \n",
    "- Can the date/time features help us predict the target value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis\n",
    "\n",
    "We are doing some basic EDA to help you familiarize with the dataset - check our results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- A number of missing values. \n",
    "- Some target values are also missing. Let's drop these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parsing datetimes \n",
    "\n",
    "- In general, datetimes are a huge pain! \n",
    "    - Think of all the formats: MM-DD-YY, DD-MM-YY, YY-MM-DD, MM/DD/YY, DD/MM/YY, DD/MM/YYYY, 20:45, 8:45am, 8:45 PM, 8:45a, 08:00, 8:10:20, .......\n",
    "  - Time zones.\n",
    "  - Daylight savings...\n",
    "- Thankfully, pandas does a pretty good job here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain = pd.to_datetime(rain_df[\"Date\"])\n",
    "dates_rain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "They are all the same format, so we can also compare dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] - dates_rain[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1] > dates_rain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dates_rain[1] - dates_rain[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also easily extract information from the date columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_year_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_rain[1].is_leap_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Above, pandas identified the date column automatically. You can also tell pandas to parse the dates when reading in the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = pd.read_csv(DATA_DIR + \"weatherAUS.csv\", parse_dates=[\"Date\"])\n",
    "rain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we re-read the csv file, let's remove the missing targets again\n",
    "rain_df = rain_df[rain_df[\"RainTomorrow\"].notna()]\n",
    "rain_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 1</font>\n",
    "- How many time series are present in this dataset? \n",
    "- Are the measurements equally spaced? Use the function provided below to help you answer this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_spacing_distribution(df, region=\"Adelaide\"):\n",
    "    \"\"\"\n",
    "    Plots the distribution of time spacing for a given region.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'Location' and 'Date'.\n",
    "        region (str): The region (e.g., location) to analyze.\n",
    "    \"\"\"\n",
    "    # Ensure 'Date' is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Filter data for the given region\n",
    "    region_data = df[df['Location'] == region]\n",
    "    \n",
    "    if region_data.empty:\n",
    "        print(f\"No data available for region: {region}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate time differences\n",
    "    time_diffs = region_data['Date'].sort_values().diff().dropna()\n",
    "    \n",
    "    # Count the frequency of each time difference\n",
    "    value_counts = time_diffs.value_counts().sort_index()\n",
    "    \n",
    "    # Display value counts\n",
    "    print(f\"Time spacing counts for {region}:\\n{value_counts}\\n\")\n",
    "    \n",
    "    # Plot the bar chart\n",
    "    plt.bar(value_counts.index.astype(str), value_counts.values, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Time Difference Distribution for {region}\")\n",
    "    plt.xlabel(\"Time Difference (days)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='red'>Question 2</font>\n",
    "\n",
    "Create train/test splits, using at least 20% samples for the test set. Remember that we should not be calling the usual `train_test_split` with shuffling because if we want to forecast, we aren't allowed to know what happened in the future!\n",
    "\n",
    "Make sure to call the resulting dataframes `train_df` and `test_df` for the rest of the notebook to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have different types of features requiring preprocessing. Let's define a preprocessor with a column transformer. \n",
    "\n",
    "This portion of the exercise is given to you, as it is not focused on using temporal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have missing data. \n",
    "- We have categorical features and numeric features. \n",
    "- To build a baseline, let's drop the date column and treat this as a usual supervised machine learning problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\n",
    "    \"MinTemp\",\n",
    "    \"MaxTemp\",\n",
    "    \"Rainfall\",\n",
    "    \"Evaporation\",\n",
    "    \"Sunshine\",\n",
    "    \"WindGustSpeed\",\n",
    "    \"WindSpeed9am\",\n",
    "    \"WindSpeed3pm\",\n",
    "    \"Humidity9am\",\n",
    "    \"Humidity3pm\",\n",
    "    \"Pressure9am\",\n",
    "    \"Pressure3pm\",\n",
    "    \"Cloud9am\",\n",
    "    \"Cloud3pm\",\n",
    "    \"Temp9am\",\n",
    "    \"Temp3pm\",\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Location\",\n",
    "    \"WindGustDir\",\n",
    "    \"WindDir9am\",\n",
    "    \"WindDir3pm\",\n",
    "    \"RainToday\",\n",
    "]\n",
    "drop_features = [\"Date\"]\n",
    "target = [\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be doing feature engineering and preprocessing features several times. So I've written a function for preprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    "):\n",
    "\n",
    "    all_features = set(numeric_features + categorical_features + drop_features + target)\n",
    "    if set(train_df.columns) != all_features:\n",
    "        print(\"Missing columns\", set(train_df.columns) - all_features)\n",
    "        print(\"Extra columns\", all_features - set(train_df.columns))\n",
    "        raise Exception(\"Columns do not match\")\n",
    "\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"median\"), StandardScaler()\n",
    "    )\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False),\n",
    "    )\n",
    "\n",
    "    preprocessor = make_column_transformer(\n",
    "        (numeric_transformer, numeric_features),\n",
    "        (categorical_transformer, categorical_features),\n",
    "        (\"drop\", drop_features),\n",
    "    )\n",
    "    preprocessor.fit(train_df)\n",
    "    ohe_feature_names = (\n",
    "        preprocessor.named_transformers_[\"pipeline-2\"]\n",
    "        .named_steps[\"onehotencoder\"]\n",
    "        .get_feature_names_out(categorical_features)\n",
    "        .tolist()\n",
    "    )\n",
    "    new_columns = numeric_features + ohe_feature_names\n",
    "\n",
    "    X_train_enc = pd.DataFrame(\n",
    "        preprocessor.transform(train_df), index=train_df.index, columns=new_columns\n",
    "    )\n",
    "    X_test_enc = pd.DataFrame(\n",
    "        preprocessor.transform(test_df), index=test_df.index, columns=new_columns\n",
    "    )\n",
    "\n",
    "    y_train = train_df[\"RainTomorrow\"]\n",
    "    y_test = test_df[\"RainTomorrow\"]\n",
    "\n",
    "    return X_train_enc, y_train, X_test_enc, y_test, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features,\n",
    "    categorical_features,\n",
    "    drop_features, target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek at X_train_enc to see the results of preprocessing\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='red'>Question 3</font>\n",
    "\n",
    "Let's treat this as a usual supervised machine learning problem and create a couple of baseline models, to get an idea of what the performance would be if we ignored the time feature.\n",
    "\n",
    "- Fit and score a `DummyClassifier` (for this exercise, score also on the test set)\n",
    "- Fit and score a `LogisticRegression` model. \n",
    "- Comment on the performance of these baselines.\n",
    "- Examine the coefficients of the logistic regression model. What features have the biggest impact on the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='red'>Question 4</font>\n",
    "\n",
    "- Use [`TimeSeriesSplit`](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split) to carry out cross-validation for a `LogisticRegression` model. \n",
    "\n",
    "Some notes before proceeding:\n",
    "- Things are a bit more complicated here because this dataset has **multiple time series**, one per location. \n",
    "- Our approach today will be to ignore the fact that we have multiple time series and just OHE the location\n",
    "- We'll have multiple measurements for a given timestamp, and that's OK.\n",
    "- But, `TimeSeriesSplit` expects the dataframe to be sorted by date so let's sort it by date before trying cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df_ordered = train_df.sort_values(by=[\"Date\"])\n",
    "y_train_ordered = train_df_ordered[\"RainTomorrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <font color='red'>Question 5</font>\n",
    "\n",
    "The feature `Date` is probably very useful to predict the target (e.g. different amounts of rain in different seasons) - let's include it as feature!\n",
    "\n",
    "This is feature engineering!\n",
    "\n",
    "Think of at least 3 ways to generate features from the `Date` column. Some examples to start:\n",
    "- Create a column for \"days since starting date\"\n",
    "- Using the month as numerical feature, or One-hot encoding it\n",
    "- One-hot encoding seasons (this requires converting months to seasons - also, remember that seasons are opposite in Australia!)\n",
    "\n",
    "After adding the new features to the dataset, re-train a `LogisticRegression` model, and see how the performance has changed. Also, observe the coefficients: are the new features important?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lag-based features\n",
    "\n",
    "Realistically, it may be helpful to know if it rained yesterday to predict if it will rain today. Let's add lagged features to our data.\n",
    "\n",
    "We can \"lag\" (or \"shift\") a time series in Pandas with the .shift() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Recreating training and test set, to start from a clean slate\n",
    "\n",
    "train_df = rain_df.query(\"Date <= 20150630\")\n",
    "test_df = rain_df.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 1 lagged column\n",
    "train_df = train_df.assign(Rainfall_lag1=train_df[\"Rainfall\"].shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[\"Date\", \"Location\", \"Rainfall\", \"Rainfall_lag1\"]][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem!** We have multiple time series here and we need to be more careful with this. \n",
    "\n",
    "When we switch from one location to another we do not want to take the value from the previous location. The function below will help with this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_lag_feature(\n",
    "    df: pd.DataFrame,\n",
    "    orig_feature: str,\n",
    "    lag: int,\n",
    "    groupby: list[str],\n",
    "    new_feature_name: str | None = None,\n",
    "    clip: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a lagged (or ahead) version of a feature, optionally per group.\n",
    "\n",
    "    Assumes df is already sorted by time within each group and has unique indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset.\n",
    "    orig_feature : str\n",
    "        Name of the column to lag.\n",
    "    lag : int\n",
    "        The lag:\n",
    "          - negative → values from the past (t-1, t-2, ...)\n",
    "          - positive → values from the future (t+1, t+2, ...)\n",
    "    groupby : list of str\n",
    "        Column(s) to group by if df contains multiple time series.\n",
    "    new_feature_name : str, optional\n",
    "        Name of the new column. If None, a name is generated automatically.\n",
    "    clip : bool, default False\n",
    "        If True, drop rows where the new feature is NaN.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new dataframe with the additional column added.\n",
    "    \"\"\"\n",
    "    if lag == 0:\n",
    "        raise ValueError(\"lag cannot be 0 (no shift). Use the original feature instead.\")\n",
    "\n",
    "    # Default name if not provided\n",
    "    if new_feature_name is None:\n",
    "        if lag < 0:\n",
    "            new_feature_name = f\"{orig_feature}_lag{abs(lag)}\"\n",
    "        else:\n",
    "            new_feature_name = f\"{orig_feature}_ahead{lag}\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Map your convention (negative=past, positive=future) to pandas shift\n",
    "    # pandas: shift(+k) → past, shift(-k) → future\n",
    "    periods = abs(lag) if lag < 0 else -lag\n",
    "\n",
    "    df[new_feature_name] = (\n",
    "        df.groupby(groupby, sort=False)[orig_feature]\n",
    "          .shift(periods)\n",
    "    )\n",
    "\n",
    "    if clip:\n",
    "        df = df.dropna(subset=[new_feature_name])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Question 6</font>\n",
    "\n",
    "- Use `create_lag_feature` to add lagged rainfall features (1 day is enough to start).\n",
    "- Discuss: would it be ok to add this feature to the test set? If the answer is yes, add it to the test set too.\n",
    "- Fit and score a `LogisticRegression` model on this new dataset. Compare the results to what was achieved before.\n",
    "- Observe the coefficients for `Rainfall` and `Rainfall_lag1`. What is their relationship with the target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Last, here are some more options for lagged features to check out:\n",
    "\n",
    "- We could also create a lagged version of the target. In fact, this dataset already has that built in! `RainToday` is the lagged version of the target `RainTomorrow`.\n",
    "- We could also create lagged version of other features, or more lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rain_df_modified = create_lag_feature(rain_df, \"Rainfall\", -1, groupby=[\"Location\"])\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", -2, groupby=[\"Location\"])\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Rainfall\", -3, groupby=[\"Location\"])\n",
    "rain_df_modified = create_lag_feature(rain_df_modified, \"Humidity3pm\", -1, groupby=[\"Location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df_modified[\n",
    "    [\n",
    "        \"Date\",\n",
    "        \"Location\",\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the pattern of `NaN` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df = rain_df_modified.query(\"Date <= 20150630\")\n",
    "test_df = rain_df_modified.query(\"Date >  20150630\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc, y_train, X_test_enc, y_test, preprocessor = preprocess_features(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    numeric_features\n",
    "    + [\"Rainfall_lag1\", \"Rainfall_lag2\", \"Rainfall_lag3\", \"Humidity3pm_lag1\"],\n",
    "    categorical_features,\n",
    "    drop_features,\n",
    "    target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coef = score_lr_print_coeff(\n",
    "    preprocessor, train_df, y_train, test_df, y_test, X_train_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coef.loc[\n",
    "    [\n",
    "        \"Rainfall\",\n",
    "        \"Rainfall_lag1\",\n",
    "        \"Rainfall_lag2\",\n",
    "        \"Rainfall_lag3\",\n",
    "        \"Humidity3pm\",\n",
    "        \"Humidity3pm_lag1\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note the pattern in the magnitude of the coefficients. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
