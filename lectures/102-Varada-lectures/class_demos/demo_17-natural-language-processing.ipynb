{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 17: Class demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), (\"..\"), \"code\"))\n",
    "\n",
    "from plotting_functions_unsup import *\n",
    "\n",
    "import IPython\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "from comat import CooccurrenceMatrix\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from preprocessing import MyPreprocessor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "DATA_DIR = os.path.join(os.path.abspath(\"../..\"), \"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kvarada/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why topic modeling?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic modeling introduction activity (~5 mins)**\n",
    "\n",
    "- Consider the following documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text\n",
       "0                                famous fashion model\n",
       "1                               elegant fashion model\n",
       "2   fashion model at famous probabilistic topic mo...\n",
       "3                         fresh elegant fashion model\n",
       "4                       famous elegant fashion model \n",
       "5                            probabilistic conference\n",
       "6                        creative probabilistic model\n",
       "7                     model diet apple kiwi nutrition\n",
       "8                                 probabilistic model\n",
       "9                               kiwi health nutrition\n",
       "10                      fresh apple kiwi health diet \n",
       "11                                   health nutrition\n",
       "12                   fresh apple kiwi juice nutrition\n",
       "13               probabilistic topic model conference\n",
       "14                           probabilistic topi model"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(DATA_DIR + \"toy_clustering.csv\")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss the following questions with your neighbour**\n",
    "1. Suppose you are asked to cluster these documents manually. How many clusters would you identify?\n",
    "2. What are the prominent words in each cluster? \n",
    "4. Are there documents which are a mixture of multiple clusters? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we learned about clustering. Let's try K-Means clustering on this data with BOW representation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 54 stored elements and shape (15, 16)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "toy_X = vec.fit_transform(toy_df[\"text\"])\n",
    "toy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>bow_kmeans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>famous fashion model</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elegant fashion model</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fashion model at famous probabilistic topic mo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fresh elegant fashion model</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>famous elegant fashion model</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probabilistic conference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>creative probabilistic model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model diet apple kiwi nutrition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>probabilistic model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kiwi health nutrition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fresh apple kiwi health diet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>health nutrition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fresh apple kiwi juice nutrition</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>probabilistic topic model conference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>probabilistic topi model</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  bow_kmeans\n",
       "0                                famous fashion model           2\n",
       "1                               elegant fashion model           2\n",
       "2   fashion model at famous probabilistic topic mo...           2\n",
       "3                         fresh elegant fashion model           2\n",
       "4                       famous elegant fashion model            2\n",
       "5                            probabilistic conference           0\n",
       "6                        creative probabilistic model           0\n",
       "7                     model diet apple kiwi nutrition           1\n",
       "8                                 probabilistic model           0\n",
       "9                               kiwi health nutrition           1\n",
       "10                      fresh apple kiwi health diet            1\n",
       "11                                   health nutrition           1\n",
       "12                   fresh apple kiwi juice nutrition           1\n",
       "13               probabilistic topic model conference           0\n",
       "14                           probabilistic topi model           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3) \n",
    "\n",
    "kmeans_bow = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_bow.fit(toy_X)\n",
    "kmeans_bow_labels = kmeans_bow.labels_\n",
    "toy_df[\"bow_kmeans\"] = kmeans_bow_labels\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any problem here? \n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling motivation\n",
    "\n",
    "- K-Means clustering gives each document a *hard assignment*; each document belongs to exactly one cluster.\n",
    "  \n",
    "- But in reality, many documents are a mixture of topics.  \n",
    "  - A news article might talk about *sports* and *politics*.  \n",
    "  - A research paper might cover *machine learning* and *linguistics*.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling: Input and output**\n",
    "\n",
    "- Input\n",
    "    - A large collection of documents\n",
    "    - A value for the hyperparameter $K$ (e.g., $K = 3$)\n",
    "- Output\n",
    "    - For each topic, what words describe that topic?  \n",
    "    - For each document, what topics are expressed by the document?\n",
    "\n",
    "![](../../img/topic_modeling_output.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/topic_modeling_output.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center> -->    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling: Some applications**\n",
    "\n",
    "- Topic modeling is a great EDA tool to get a sense of what's going on in a large corpus. \n",
    "- Some examples\n",
    "    - If you want to pull documents related to a particular lawsuit. \n",
    "    - You want to examine people's sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling: Input**\n",
    "\n",
    "![](../../img/TM_science_articles.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_science_articles.png\" height=\"2000\" width=\"2000\">  -->\n",
    "<!-- </center>     -->\n",
    "    \n",
    "Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling: output**\n",
    "\n",
    "![](../../img/TM_topics.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics.png\" height=\"900\" width=\"900\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Topic modeling: output with interpretation**\n",
    "- Assigning labels is a human thing. \n",
    "\n",
    "![](../../img/TM_topics_with_labels.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_topics_with_labels.png\" height=\"800\" width=\"800\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's presentation](http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**LDA topics in Yale Law Journal**\n",
    "\n",
    "![](../../img/TM_yale_law_journal.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_yale_law_journal.png\" height=\"1500\" width=\"1500\">  -->\n",
    "<!-- </center>     -->\n",
    "\n",
    "(Credit: [David Blei's paper](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA topics in social media\n",
    "\n",
    "![](../../img/TM_health_topics_social_media.png)\n",
    "\n",
    "<!-- <center> -->\n",
    "<!-- <img src=\"img/TM_health_topics_social_media.png\" height=\"1300\" width=\"1300\">  -->\n",
    "<!-- </center> -->\n",
    "\n",
    "(Credit: [Health topics in social media](https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, I will demonstrate how to perform topic modeling using the **Latent Dirichlet Allocation** model implemented in `sklearn`. We won't delve into the inner workings of the model, as it falls outside the scope of this course. Instead, our objective is to understand how to apply it to your specific problems and comprehend the model's input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling toy example\n",
    "\n",
    "Let's work with a toy example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fashion model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>fresh fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>creative fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>famous fashion model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>probabilistic model pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>fashion model probabilistic topic model confer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>kiwi health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>fresh apple health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>creative health nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>hidden markov model probabilistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>probabilistic topic model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>apple kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>fresh kiwi health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>apple kiwi nutrition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                               text\n",
       "0        1                               famous fashion model\n",
       "1        2                             fashion model pattern \n",
       "2        3  fashion model probabilistic topic model confer...\n",
       "3        4                               famous fashion model\n",
       "4        5                                fresh fashion model\n",
       "5        6                               famous fashion model\n",
       "6        7                               famous fashion model\n",
       "7        8                               famous fashion model\n",
       "8        9                               famous fashion model\n",
       "9       10                             creative fashion model\n",
       "10      11                               famous fashion model\n",
       "11      12                               famous fashion model\n",
       "12      13  fashion model probabilistic topic model confer...\n",
       "13      14                          probabilistic topic model\n",
       "14      15                        probabilistic model pattern\n",
       "15      16                          probabilistic topic model\n",
       "16      17                          probabilistic topic model\n",
       "17      18                          probabilistic topic model\n",
       "18      19                          probabilistic topic model\n",
       "19      20                          probabilistic topic model\n",
       "20      21                          probabilistic topic model\n",
       "21      22  fashion model probabilistic topic model confer...\n",
       "22      23                               apple kiwi nutrition\n",
       "23      24                              kiwi health nutrition\n",
       "24      25                                 fresh apple health\n",
       "25      26                          probabilistic topic model\n",
       "26      27                          creative health nutrition\n",
       "27      28                          probabilistic topic model\n",
       "28      29                          probabilistic topic model\n",
       "29      30                  hidden markov model probabilistic\n",
       "30      31                          probabilistic topic model\n",
       "31      32                          probabilistic topic model\n",
       "32      33                               apple kiwi nutrition\n",
       "33      34                                  apple kiwi health\n",
       "34      35                               apple kiwi nutrition\n",
       "35      36                                  fresh kiwi health\n",
       "36      37                               apple kiwi nutrition\n",
       "37      38                               apple kiwi nutrition\n",
       "38      39                               apple kiwi nutrition"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df = pd.read_csv(DATA_DIR + \"toy_lda_data.csv\")\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input to the LDA topic model is bag-of-words representation of text.\n",
    "- Let's create bag-of-words representation of \"text\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 124 stored elements and shape (39, 15)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "toy_X = vec.fit_transform(toy_df[\"text\"])\n",
    "toy_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['apple', 'conference', 'creative', 'famous', 'fashion', 'fresh',\n",
       "       'health', 'hidden', 'kiwi', 'markov', 'model', 'nutrition',\n",
       "       'pattern', 'probabilistic', 'topic'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vec.get_feature_names_out() # vocabulary\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to create a topic model with sklearn's `LatentDirichletAllocation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3 # number of topics\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "lda.fit(toy_X) \n",
    "document_topics = lda.transform(toy_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once we have a fitted model we can get the word-topic association and document-topic association  \n",
    "- Word-topic association\n",
    "    - `lda.components_` gives us the weights associated with each word for each topic. In other words, it tells us which word is important for which topic. \n",
    "- Document-topic association    \n",
    "    - Calling transform on the data gives us document-topic association. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33380754,  3.31038074,  0.33476534,  0.33397112,  0.36695134,\n",
       "         0.33439238,  0.33381373,  0.35771821,  0.33380649,  0.35771821,\n",
       "        17.78521263,  0.33380761,  0.3573886 , 17.31634363, 15.32791718],\n",
       "       [ 8.33224516,  0.33400489,  2.2173627 ,  0.33411086,  0.33732465,\n",
       "         3.28753559,  5.33223002,  0.33435326,  9.33224759,  0.33435326,\n",
       "         0.33797555,  8.3322447 ,  0.33462759,  0.33440682,  0.33425967],\n",
       "       [ 0.3339473 ,  0.35561437,  0.44787197,  8.33191802, 14.29572402,\n",
       "         0.37807203,  0.33395626,  1.30792853,  0.33394593,  1.30792853,\n",
       "        13.87681182,  0.33394769,  2.30798381,  0.34924955,  0.33782315]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (3, 15)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "x": [
          "apple",
          "conference",
          "creative",
          "famous",
          "fashion",
          "fresh",
          "health",
          "hidden",
          "kiwi",
          "markov",
          "model",
          "nutrition",
          "pattern",
          "probabilistic",
          "topic"
         ],
         "xaxis": "x",
         "y": [
          "topic 0",
          "topic 1",
          "topic 2"
         ],
         "yaxis": "y",
         "z": {
          "_inputArray": [
           {
            "0": 0.3338075384293074,
            "1": 3.3103807364695625,
            "10": 17.78521262941694,
            "11": 0.33380761205184795,
            "12": 0.3573886039463434,
            "13": 17.31634363323813,
            "14": 15.327917175591686,
            "2": 0.3347653361777077,
            "3": 0.33397111684249164,
            "4": 0.36695133607624736,
            "5": 0.33439237889442985,
            "6": 0.3338137262597073,
            "7": 0.3577182090397569,
            "8": 0.3338064864890934,
            "9": 0.3577182090409376
           },
           {
            "0": 8.332245163308864,
            "1": 0.33400488990063776,
            "10": 0.3379755519908432,
            "11": 8.332244700237965,
            "12": 0.334627588072758,
            "13": 0.33440682170070085,
            "14": 0.33425967292590886,
            "2": 2.217362696727862,
            "3": 0.33411086009921837,
            "4": 0.3373246487231333,
            "5": 3.2875355877178407,
            "6": 5.332230015393538,
            "7": 0.33435326369261237,
            "8": 9.332247586680836,
            "9": 0.33435326369261703
           },
           {
            "0": 0.3339472982618195,
            "1": 0.3556143736297804,
            "10": 13.876811818592186,
            "11": 0.33394768771017724,
            "12": 2.307983807980878,
            "13": 0.34924954506115014,
            "14": 0.337823151482388,
            "2": 0.4478719670944038,
            "3": 8.331918023058277,
            "4": 14.2957240152006,
            "5": 0.37807203338771006,
            "6": 0.3339562583467434,
            "7": 1.3079285272676102,
            "8": 0.3339459268300608,
            "9": 1.307928527266425
           }
          ],
          "bdata": "rZMtSxpd1T+zj0PlqHsKQM3urZbLbNU/B2opZMhf1T918ep0IXzXP6CG2EqvZtU/GnJPPzRd1T/JmkDq2uTWP95zquEVXdU/3+1A6trk1j9+wOOxA8kxQGbJOpoaXdU/uGh6c3Tf1j87Dnfl+1AxQAztkcLkpy5A3VS9CRyqIEDP8rQLVmDVP0OITqcovQFAL1VXhBJi1T+KopkfupbVP2B5TXXfTApAlHXrGjRUFUB5uDc7C2bVPzDwDVscqiJAzbg3Owtm1T9tfKk1ZKHVPzCVM/obqiBAoyjH1Ilq1T9R4rDe62bVP+yo5a6CZNU/CdAnfWRf1T89ji7KYsLWP0DN3S7vqdw///m7KPGpIEBV21sjaZcsQAOtuwpVMtg/2zP5EYpf1T/T6qF2Ru30P3OFl7xeX9U/+dWhdkbt9D8PM4t67cArQOSPUh9mX9U/ps33NsB2AkCcjYvDGlrWP2W03P7kntU/",
          "dtype": "f8",
          "shape": "3, 15"
         }
        }
       ],
       "layout": {
        "autosize": false,
        "coloraxis": {
         "colorscale": [
          [
           0,
           "#440154"
          ],
          [
           0.1111111111111111,
           "#482878"
          ],
          [
           0.2222222222222222,
           "#3e4989"
          ],
          [
           0.3333333333333333,
           "#31688e"
          ],
          [
           0.4444444444444444,
           "#26828e"
          ],
          [
           0.5555555555555556,
           "#1f9e89"
          ],
          [
           0.6666666666666666,
           "#35b779"
          ],
          [
           0.7777777777777778,
           "#6ece58"
          ],
          [
           0.8888888888888888,
           "#b5de2b"
          ],
          [
           1,
           "#fde725"
          ]
         ]
        },
        "height": 600,
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "width": 800,
        "xaxis": {
         "anchor": "y",
         "autorange": true,
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "range": [
          -0.5,
          14.5
         ],
         "scaleanchor": "y",
         "side": "top",
         "tickangle": -60,
         "title": {
          "text": "Features"
         },
         "type": "category"
        },
        "yaxis": {
         "anchor": "x",
         "autorange": true,
         "constrain": "domain",
         "domain": [
          0.3630434782608696,
          0.6369565217391304
         ],
         "range": [
          2.5,
          -0.5
         ],
         "title": {
          "text": "Principal Components"
         },
         "type": "category"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "plot_lda_w_vectors(lda.components_, ['topic 0', 'topic 1', 'topic 2'], vocab, width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the words with highest weights for each topic more systematically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  0, 11,  6,  3,  5,  2, 12,  7,  9,  4,  1, 14, 13, 10],\n",
       "       [ 1,  3, 14,  7,  9, 13, 12,  4, 10,  2,  5,  6, 11,  0,  8],\n",
       "       [ 8,  0, 11,  6, 14, 13,  1,  5,  2,  9,  7, 12,  3, 10,  4]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(lda.components_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "model         kiwi          fashion       \n",
      "probabilistic apple         model         \n",
      "topic         nutrition     famous        \n",
      "conference    health        pattern       \n",
      "fashion       fresh         hidden        \n",
      "markov        creative      markov        \n",
      "hidden        model         creative      \n",
      "pattern       fashion       fresh         \n",
      "creative      pattern       conference    \n",
      "fresh         probabilistic probabilistic \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(3),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how we can interpret the topics\n",
    "    - Topic 0 $\\rightarrow$ ML modeling \n",
    "    - Topic 1 $\\rightarrow$ fruit and nutrition\n",
    "    - Topic 2 $\\rightarrow$ fashion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at distribution of topics for a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'famous fashion model'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08791477, 0.08338644, 0.82869879])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is made up of \n",
    "- ~83% topic 2\n",
    "- ~9% topic 0 \n",
    "- ~8% topic 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Topic modeling pipeline \n",
    "\n",
    "- Above we worked with toy data. In the real world, we usually need to preprocess the data before passing it to LDA. \n",
    "- Here are typical steps if you want to carry out topic modeling on real-world data. \n",
    "    - Preprocess your corpus. \n",
    "    - Train LDA.\n",
    "    - Interpret your topics.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) is the capability...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey in North America)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1              unsupervised learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "\n",
       "                                                text  \n",
       "0  Artificial intelligence (AI) is the capability...  \n",
       "1  In machine learning, supervised learning (SL) ...  \n",
       "2  The Supreme Court of Canada (SCC; French: Cour...  \n",
       "3  In many Commonwealth jurisdictions, the phrase...  \n",
       "4  Canadian constitutional law (French: droit con...  \n",
       "5  Ice hockey (or simply hockey in North America)...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "queries = [\n",
    "    \"Artificial Intelligence\",\n",
    "    \"unsupervised learning\",\n",
    "    \"Supreme Court of Canada\",\n",
    "    \"Peace, Order, and Good Government\",\n",
    "    \"Canadian constitutional law\",\n",
    "    \"ice hockey\",\n",
    "]\n",
    "wiki_dict = {\"wiki query\": [], \"text\": []}\n",
    "for i in range(len(queries)):\n",
    "    wiki_dict[\"text\"].append(wikipedia.page(queries[i]).content)\n",
    "    wiki_dict[\"wiki query\"].append(queries[i])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Preprocessing the corpus**\n",
    "\n",
    "- **Preprocessing is crucial!**\n",
    "- Tokenization, converting text to lower case\n",
    "- Removing punctuation and stopwords\n",
    "- Discarding words with length < threshold or word frequency < threshold        \n",
    "- Possibly lemmatization: Consider the lemmas instead of inflected forms. \n",
    "- Depending upon your application, restrict to specific part of speech;\n",
    "    * For example, only consider nouns, verbs, and adjectives\n",
    "\n",
    "Check out [AppendixC](../../notes/AppendixC.ipynb) for basic preprocessing in NLP. \n",
    "\n",
    "We'll use [`spaCy`](https://spacy.io/) for preprocessing. Check out available token attributes [here](https://spacy.io/usage/rule-based-matching#adding-patterns-attributes). \n",
    "\n",
    "Even though you have `spacy` installed, you will need to install the following pre-trained `spacy` model in the course environment. \n",
    "\n",
    "```python -m spacy download en_core_web_md```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (\n",
    "            token.is_stop == False  # Check if it's not a stopword\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wiki_df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(wiki_df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki query</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>Artificial intelligence (AI) is the capability...</td>\n",
       "      <td>artificial intelligence capability computation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unsupervised learning</td>\n",
       "      <td>In machine learning, supervised learning (SL) ...</td>\n",
       "      <td>machine learning supervised learning type mach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Supreme Court of Canada</td>\n",
       "      <td>The Supreme Court of Canada (SCC; French: Cour...</td>\n",
       "      <td>supreme court canada scc french cour suprme c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peace, Order, and Good Government</td>\n",
       "      <td>In many Commonwealth jurisdictions, the phrase...</td>\n",
       "      <td>commonwealth jurisdiction phrase peace order g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Canadian constitutional law</td>\n",
       "      <td>Canadian constitutional law (French: droit con...</td>\n",
       "      <td>canadian constitutional law french droit const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ice hockey</td>\n",
       "      <td>Ice hockey (or simply hockey in North America)...</td>\n",
       "      <td>ice hockey hockey north america team sport pla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          wiki query  \\\n",
       "0            Artificial Intelligence   \n",
       "1              unsupervised learning   \n",
       "2            Supreme Court of Canada   \n",
       "3  Peace, Order, and Good Government   \n",
       "4        Canadian constitutional law   \n",
       "5                         ice hockey   \n",
       "\n",
       "                                                text  \\\n",
       "0  Artificial intelligence (AI) is the capability...   \n",
       "1  In machine learning, supervised learning (SL) ...   \n",
       "2  The Supreme Court of Canada (SCC; French: Cour...   \n",
       "3  In many Commonwealth jurisdictions, the phrase...   \n",
       "4  Canadian constitutional law (French: droit con...   \n",
       "5  Ice hockey (or simply hockey in North America)...   \n",
       "\n",
       "                                             text_pp  \n",
       "0  artificial intelligence capability computation...  \n",
       "1  machine learning supervised learning type mach...  \n",
       "2  supreme court canada scc french cour suprme c...  \n",
       "3  commonwealth jurisdiction phrase peace order g...  \n",
       "4  canadian constitutional law french droit const...  \n",
       "5  ice hockey hockey north america team sport pla...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(stop_words='english')\n",
    "X = vec.fit_transform(wiki_df[\"text_pp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (3, 4095)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       \n",
      "--------      --------      --------      \n",
      "displaystyle  court         hockey        \n",
      "algorithm     intelligence  player        \n",
      "learning      problem       team          \n",
      "law           artificial    ice           \n",
      "function      machine       league        \n",
      "training      human         play          \n",
      "learn         use           puck          \n",
      "provincial    decision      game          \n",
      "court         include       penalty       \n",
      "federal       learning      canada        \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(3),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out some recent topic modeling tools\n",
    "- [Topic2Vec](https://top2vec.readthedocs.io/en/stable/Top2Vec.html)\n",
    "- [BERTopic](https://maartengr.github.io/BERTopic/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text representations and word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation \n",
    "\n",
    "Do large language models such as ChatGPT *understand* your questions to some extent and provide useful responses?\n",
    "\n",
    "What would it take for a machine to \"understand\" language?  \n",
    "A first step is to find a way to represent text, numbers that capture meaning.\n",
    "\n",
    "So far, we have seen **bag-of-words (BoW)** representation using `CountVectorizer` in `sklearn`.  \n",
    "\n",
    "Let's quickly recall what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>terrible</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amazing  is  movie  terrible  this\n",
       "0        1   1      1         0     1\n",
       "1        0   1      1         1     1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "docs = [\"This movie is amazing\", \"This movie is terrible\"]\n",
    "vec = CountVectorizer()\n",
    "pd.DataFrame(vec.fit_transform(docs).toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some limitations of Bag-of-Words? \n",
    "\n",
    "- Sparse, high-dimensional vectors\n",
    "\n",
    "- Only capture word frequency\n",
    "\n",
    "- Ignore word order and context\n",
    "\n",
    "- Do not put similar words (e.g., happy, joyful) close together\n",
    "\n",
    "BoW represents documents, but it treats each word as an independent token, \n",
    "there's no notion of word meaning or relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the lecture, we are going to go one step back and talk about word representations. Why care about word representations?\n",
    "\n",
    "- Words are the basic building blocks of language; the smallest units that carry meaning.\n",
    "\n",
    "- To truly understand a document, a model must first understand the meaning of the words in it.\n",
    "- If we can represent each word in a way that captures its meaning,\n",
    "then we can combine these representations to understand larger pieces of text (sentences, paragraphs, documents).\n",
    "- In other words, to represent text meaningfully, we must start with word meaning.\n",
    "\n",
    "This brings us to a key question: How can we represent the meaning of individual words using numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity: Context and word meaning**\n",
    "\n",
    "Pair up with a neighbor and try to guess the meanings of the following made-up words: flibbertigibbet and groak.\n",
    "\n",
    "> 1. The plot twist was totally unexpected, making it a **flibbertigibbet** experience.\n",
    "> 2. Despite its **groak** special effects, the storyline captivated my attention till the end.\n",
    "> 3. I found the character development rather **groak**, failing to evoke empathy.\n",
    "> 4. The cinematography is **flibbertigibbet**, showcasing breathtaking landscapes.\n",
    "> 5. Sadly, the movie's potential was overshadowed by its **groak** pacing.\n",
    "\n",
    "Discussion:\n",
    "\n",
    "- How did you infer the meanings of these words?\n",
    "- Which words or phrases helped you?\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the previous activity, you guessed the meaning of flibbertigibbet and groak based on surrounding words.\n",
    "- Thats exactly what machines do when they learn from text.\n",
    "- This idea is called **distributional hypothesis**. \n",
    "\n",
    "<blockquote> \n",
    "    <p>You shall know a word by the company it keeps.</p>\n",
    "    <footer>Firth, 1957</footer>        \n",
    "</blockquote>\n",
    "\n",
    "In other words, words that appear in similar contexts tend to have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings: The idea\n",
    "\n",
    "Building on this idea, modern NLP systems learn word embeddings, dense vector representations of words that capture these contextual relationships.\n",
    "\n",
    "![](../../img/t-SNE_word_embeddings.png)\n",
    " \n",
    "(Attribution: [Jurafsky and Martin 3rd edition](https://web.stanford.edu/~jurafsky/slp3/))\n",
    "\n",
    "Example:\n",
    "> \"The plot twist was totally unexpected, making it a **flibbertigibbet** experience.\"\n",
    "\n",
    "> \"The plot twist was totally unexpected, making it a **delightful** experience.\"\n",
    "\n",
    "The goal: words like **flibbertigibbet** and **delightful** should be close in the embedding space.\n",
    "\n",
    "Word embeddings are built on the distributional hypothesis. They mathematically capture the idea that context defines meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained word embeddings\n",
    "\n",
    "Creating these representations on your own is resource intensive. So people typically use \"pretrained\" embeddings. \n",
    "A number of pre-trained word embeddings are available. The most popular ones are:  \n",
    "\n",
    "- [word2vec](https://code.google.com/archive/p/word2vec/)\n",
    "    * trained on several corpora using the word2vec algorithm \n",
    "- [wikipedia2vec](https://wikipedia2vec.github.io/wikipedia2vec/pretrained/)\n",
    "    * pretrained embeddings for 12 languages \n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using [the fastText algorithm](http://aclweb.org/anthology/Q17-1010)\n",
    "    * published by Facebook    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use pretrained embeddings**\n",
    "\n",
    "Let's try Google News pre-trained embeddings.  \n",
    "\n",
    "- You can download pre-trained embeddings from their original source. \n",
    "- `Gensim` provides an api to conveniently load them. You need to install the `gensim` package in the course environment. \n",
    "\n",
    "```conda install conda-forge::gensim```\n",
    "\n",
    "If you get errors when you import `gensim`, try to install the following in the course environment.  \n",
    "```pip install --upgrade gensim scipy```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "print(list(api.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the demonstration purpose, we'll use `word2vec-google-news-300`. This model used to be available through `gensim.downloader`, but due to licensing and size (1.5+ GB), it's no longer hosted on the default Gensim data server.\n",
    "\n",
    "So I have manually download them from [this source](https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300) and then loaded them locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It'll take a while to run this when you try it out for the first time.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_path = DATA_DIR + \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "google_news_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of vocabulary: \", len(google_news_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- `google_news_vectors` above has 300 dimensional word vectors for 3,000,000 unique words/phrases from Google news. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What can we do with these word vectors?**\n",
    "\n",
    "- Let's examine word vector for the word UBC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3828125 , -0.18066406,  0.10644531,  0.4296875 ,  0.21582031,\n",
       "       -0.10693359,  0.13476562, -0.08740234, -0.14648438, -0.09619141,\n",
       "        0.02807617,  0.01409912, -0.12890625, -0.21972656, -0.41210938,\n",
       "       -0.1875    , -0.11914062, -0.22851562,  0.19433594, -0.08642578],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors[\"UBC\"][:20]  # Representation of the word UBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors[\"UBC\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a short and a dense (we do not see any zeros) vector! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Finding similar words**\n",
    "\n",
    "- Given word $w$, search in the vector space for the word closest to $w$ as measured by cosine similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('UVic', 0.7886475920677185),\n",
       " ('SFU', 0.7588528394699097),\n",
       " ('Simon_Fraser', 0.7356574535369873),\n",
       " ('UFV', 0.688043475151062),\n",
       " ('VIU', 0.6778583526611328),\n",
       " ('Kwantlen', 0.6771429181098938),\n",
       " ('UBCO', 0.6734487414360046),\n",
       " ('UPEI', 0.673112690448761),\n",
       " ('UBC_Okanagan', 0.6709133386611938),\n",
       " ('Lakehead_University', 0.6622507572174072)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"UBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('info', 0.7363681793212891),\n",
       " ('infomation', 0.680029571056366),\n",
       " ('infor_mation', 0.673384964466095),\n",
       " ('informaiton', 0.6639008522033691),\n",
       " ('informa_tion', 0.660125732421875),\n",
       " ('informationon', 0.633933424949646),\n",
       " ('informationabout', 0.6320978999137878),\n",
       " ('Information', 0.6186580657958984),\n",
       " ('informaion', 0.6093292832374573),\n",
       " ('details', 0.6063088774681091)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract all documents containing words similar to **information**, you could use this information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Google News embeddings also support multi-word phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alberta', 0.6111123561859131),\n",
       " ('canadian', 0.6086404323577881),\n",
       " ('ontario', 0.6031432151794434),\n",
       " ('erik', 0.5993571281433105),\n",
       " ('dominican_republic', 0.5925410985946655),\n",
       " ('costco', 0.5824530124664307),\n",
       " ('rhode_island', 0.5804311633110046),\n",
       " ('dreampharmaceuticals', 0.5755444169044495),\n",
       " ('canada', 0.5630921721458435),\n",
       " ('austin', 0.5623061656951904)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.most_similar(\"british_columbia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why \"erik\" and \"costco\" show up for \"british_columbia\"? Note that word embeddings capture how words are used, not what they mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Finding similarity scores between words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.27610135)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.similarity(\"Canada\", \"hockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0019627889)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_news_vectors.similarity(\"Japan\", \"hockey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between height and tall is 0.473\n",
      "The similarity between height and official is 0.002\n",
      "The similarity between pineapple and mango is 0.668\n",
      "The similarity between pineapple and juice is 0.418\n",
      "The similarity between sun and robot is 0.029\n",
      "The similarity between GPU and hummus is 0.094\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [\n",
    "    (\"height\", \"tall\"),\n",
    "    (\"height\", \"official\"),\n",
    "    (\"pineapple\", \"mango\"),\n",
    "    (\"pineapple\", \"juice\"),\n",
    "    (\"sun\", \"robot\"),\n",
    "    (\"GPU\", \"hummus\"),\n",
    "]\n",
    "for pair in word_pairs:\n",
    "    print(\n",
    "        \"The similarity between %s and %s is %0.3f\"\n",
    "        % (pair[0], pair[1], google_news_vectors.similarity(pair[0], pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting reasonable word similarity scores!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success of word2vec**\n",
    "\n",
    "- This analogy example often comes up when people talk about word2vec, which was used by the authors of this method. \n",
    "- **MAN : KING :: WOMAN : ?**\n",
    "    - What is the word that is similar to **WOMAN** in the same sense as **KING** is similar to **MAN**?\n",
    "- Perform a simple algebraic operations with the vector representation of words.\n",
    "    $\\vec{X} = \\vec{\\text{KING}}  \\vec{\\text{MAN}} + \\vec{\\text{WOMAN}}$\n",
    "- Search in the vector space for the word closest to $\\vec{X}$ measured by cosine distance.\n",
    "\n",
    "<img src=\"../../img/word_analogies1.png\" width=\"400\" height=\"400\">\n",
    "    \n",
    "(Credit: Mikolov et al. 2013)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=google_news_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>queen</td>\n",
       "      <td>0.711819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>monarch</td>\n",
       "      <td>0.618967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>princess</td>\n",
       "      <td>0.590243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>crown_prince</td>\n",
       "      <td>0.549946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>prince</td>\n",
       "      <td>0.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>kings</td>\n",
       "      <td>0.523684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Queen_Consort</td>\n",
       "      <td>0.523595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>queens</td>\n",
       "      <td>0.518113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sultan</td>\n",
       "      <td>0.509859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>monarchy</td>\n",
       "      <td>0.508741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0          queen  0.711819\n",
       "1        monarch  0.618967\n",
       "2       princess  0.590243\n",
       "3   crown_prince  0.549946\n",
       "4         prince  0.537732\n",
       "5          kings  0.523684\n",
       "6  Queen_Consort  0.523595\n",
       "7         queens  0.518113\n",
       "8         sultan  0.509859\n",
       "9       monarchy  0.508741"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"king\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montreal : Canadiens :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canucks</td>\n",
       "      <td>0.821327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vancouver_Canucks</td>\n",
       "      <td>0.750401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Calgary_Flames</td>\n",
       "      <td>0.705471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leafs</td>\n",
       "      <td>0.695783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maple_Leafs</td>\n",
       "      <td>0.691617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thrashers</td>\n",
       "      <td>0.687504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avs</td>\n",
       "      <td>0.681716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sabres</td>\n",
       "      <td>0.665307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blackhawks</td>\n",
       "      <td>0.664625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Habs</td>\n",
       "      <td>0.661023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Analogy word     Score\n",
       "0            Canucks  0.821327\n",
       "1  Vancouver_Canucks  0.750401\n",
       "2     Calgary_Flames  0.705471\n",
       "3              Leafs  0.695783\n",
       "4        Maple_Leafs  0.691617\n",
       "5          Thrashers  0.687504\n",
       "6                Avs  0.681716\n",
       "7             Sabres  0.665307\n",
       "8         Blackhawks  0.664625\n",
       "9               Habs  0.661023"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"Montreal\", \"Canadiens\", \"Vancouver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toronto : UofT :: Vancouver : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SFU</td>\n",
       "      <td>0.579245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UVic</td>\n",
       "      <td>0.576921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UBC</td>\n",
       "      <td>0.571431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Simon_Fraser</td>\n",
       "      <td>0.543464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Langara_College</td>\n",
       "      <td>0.541347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UVIC</td>\n",
       "      <td>0.520495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Grant_MacEwan</td>\n",
       "      <td>0.517273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UFV</td>\n",
       "      <td>0.514150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ubyssey</td>\n",
       "      <td>0.510421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kwantlen</td>\n",
       "      <td>0.503807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Analogy word     Score\n",
       "0              SFU  0.579245\n",
       "1             UVic  0.576921\n",
       "2              UBC  0.571431\n",
       "3     Simon_Fraser  0.543464\n",
       "4  Langara_College  0.541347\n",
       "5             UVIC  0.520495\n",
       "6    Grant_MacEwan  0.517273\n",
       "7              UFV  0.514150\n",
       "8          Ubyssey  0.510421\n",
       "9         Kwantlen  0.503807"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"Toronto\", \"UofT\", \"Vancouver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gauss : mathematician :: Bob_Dylan : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>singer_songwriter_Bob_Dylan</td>\n",
       "      <td>0.520782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poet</td>\n",
       "      <td>0.501191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pete_Seeger</td>\n",
       "      <td>0.497143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joan_Baez</td>\n",
       "      <td>0.492307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sitarist_Ravi_Shankar</td>\n",
       "      <td>0.491968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bluesman</td>\n",
       "      <td>0.490930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jazz_musician</td>\n",
       "      <td>0.489593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Joni_Mitchell</td>\n",
       "      <td>0.487740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Billie_Holiday</td>\n",
       "      <td>0.486664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Johnny_Cash</td>\n",
       "      <td>0.485722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Analogy word     Score\n",
       "0  singer_songwriter_Bob_Dylan  0.520782\n",
       "1                         poet  0.501191\n",
       "2                  Pete_Seeger  0.497143\n",
       "3                    Joan_Baez  0.492307\n",
       "4        sitarist_Ravi_Shankar  0.491968\n",
       "5                     bluesman  0.490930\n",
       "6                jazz_musician  0.489593\n",
       "7                Joni_Mitchell  0.487740\n",
       "8               Billie_Holiday  0.486664\n",
       "9                  Johnny_Cash  0.485722"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"Gauss\", \"mathematician\", \"Bob_Dylan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you can imagine these models being useful in many meaning-related tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Examples of semantic and syntactic relationships**\n",
    "\n",
    "\n",
    "<img src=\"../../img/word_analogies2.png\" width=\"800\" height=\"800\">\n",
    "\n",
    "(Credit: Mikolov 2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implicit biases and stereotypes in word embeddings**\n",
    "\n",
    "Embeddings reflect biases in the data they are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : computer_programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.562712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housewife</td>\n",
       "      <td>0.510505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>graphic_designer</td>\n",
       "      <td>0.505180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>schoolteacher</td>\n",
       "      <td>0.497949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.493489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>paralegal</td>\n",
       "      <td>0.492551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>registered_nurse</td>\n",
       "      <td>0.490797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saleswoman</td>\n",
       "      <td>0.488163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>electrical_engineer</td>\n",
       "      <td>0.479773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanical_engineer</td>\n",
       "      <td>0.475540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Analogy word     Score\n",
       "0            homemaker  0.562712\n",
       "1            housewife  0.510505\n",
       "2     graphic_designer  0.505180\n",
       "3        schoolteacher  0.497949\n",
       "4        businesswoman  0.493489\n",
       "5            paralegal  0.492551\n",
       "6     registered_nurse  0.490797\n",
       "7           saleswoman  0.488163\n",
       "8  electrical_engineer  0.479773\n",
       "9  mechanical_engineer  0.475540"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"computer_programmer\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../../img/eva-srsly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Embeddings reflect gender stereotypes present in broader society.\n",
    "- They may also amplify these stereotypes because of their widespread usage. \n",
    "- See the paper [Man is to Computer Programmer as Woman is to ...](http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the modern embeddings are de-biased for some obvious biases. For example, we won't see this with `glove_wiki_vectors`. We changed \"computer_programmer\" to \"programmer\" because \n",
    "\"computer_programmer\" is not in the vocabulary of `glove_wiki_vectors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_wiki_vectors = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : programmer :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>programmers</td>\n",
       "      <td>0.497601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freelance</td>\n",
       "      <td>0.417259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>educator</td>\n",
       "      <td>0.403169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>businesswoman</td>\n",
       "      <td>0.392910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>designer</td>\n",
       "      <td>0.392894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>translator</td>\n",
       "      <td>0.385843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>technician</td>\n",
       "      <td>0.375108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>computer</td>\n",
       "      <td>0.374914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>animator</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>homemaker</td>\n",
       "      <td>0.367547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Analogy word     Score\n",
       "0    programmers  0.497601\n",
       "1      freelance  0.417259\n",
       "2       educator  0.403169\n",
       "3  businesswoman  0.392910\n",
       "4       designer  0.392894\n",
       "5     translator  0.385843\n",
       "6     technician  0.375108\n",
       "7       computer  0.374914\n",
       "8       animator  0.367700\n",
       "9      homemaker  0.367547"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"programmer\", \"woman\", model = glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Other popular methods to get embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[fastText](https://fasttext.cc/)**\n",
    "\n",
    "- NLP library by Facebook research  \n",
    "- Includes an algorithm which is an extension to word2vec\n",
    "- Helps deal with unknown words elegantly\n",
    "- Breaks words into several n-gram subwords \n",
    "- Example: trigram sub-words for *berry* are *ber*, *err*, *rry*\n",
    "- Embedding(*berry*) = embedding(*ber*) + embedding(*err*) + embedding(rry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**(Optional) [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/projects/glove/)**\n",
    "- Starts with the co-occurrence matrix\n",
    "    - Co-occurrence can be interpreted as an indicator of semantic proximity of words\n",
    "- Takes advantage of global count statistics    \n",
    "- Predicts co-occurrence ratios\n",
    "- Loss based on word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond words: sentence embeddings\n",
    "\n",
    "- Word embeddings capture individual word meanings.\n",
    "- But what about sentences or paragraphs?\n",
    "- Modern deep learning models represent whole texts using sentence embeddings, where the meaning of a sentence is captured in a single vector. This is what you used in HW6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kvarada/miniforge3/envs/cpsc330/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning:\n",
      "\n",
      "`encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The cat sat on the mat.</th>\n",
       "      <th>A feline rested on the rug.</th>\n",
       "      <th>I love teaching you machine learning.</th>\n",
       "      <th>Natural language processing is fascinating.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>The cat sat on the mat.</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.556</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A feline rested on the rug.</th>\n",
       "      <td>0.556</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I love teaching you machine learning.</th>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Natural language processing is fascinating.</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.462</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             The cat sat on the mat.  \\\n",
       "The cat sat on the mat.                                        1.000   \n",
       "A feline rested on the rug.                                    0.556   \n",
       "I love teaching you machine learning.                         -0.009   \n",
       "Natural language processing is fascinating.                    0.011   \n",
       "\n",
       "                                             A feline rested on the rug.  \\\n",
       "The cat sat on the mat.                                            0.556   \n",
       "A feline rested on the rug.                                        1.000   \n",
       "I love teaching you machine learning.                             -0.009   \n",
       "Natural language processing is fascinating.                        0.030   \n",
       "\n",
       "                                             I love teaching you machine learning.  \\\n",
       "The cat sat on the mat.                                                     -0.009   \n",
       "A feline rested on the rug.                                                 -0.009   \n",
       "I love teaching you machine learning.                                        1.000   \n",
       "Natural language processing is fascinating.                                  0.462   \n",
       "\n",
       "                                             Natural language processing is fascinating.  \n",
       "The cat sat on the mat.                                                            0.011  \n",
       "A feline rested on the rug.                                                        0.030  \n",
       "I love teaching you machine learning.                                              0.462  \n",
       "Natural language processing is fascinating.                                        1.000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sentences = [\n",
    "\"The cat sat on the mat.\",\n",
    "\"A feline rested on the rug.\",\n",
    "\"I love teaching you machine learning.\",\n",
    "\"Natural language processing is fascinating.\"\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compute pairwise cosine similarities\n",
    "similarities = util.cos_sim(embeddings, embeddings).numpy()\n",
    "\n",
    "# Create a table\n",
    "df_sim = pd.DataFrame(similarities, index=sentences, columns=sentences)\n",
    "df_sim.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Optional) Quick peek: Getting an embedding from an LLM API**\n",
    "\n",
    "Under the hood, all large language models (LLMs) represent words and sentences as vectors, also called embeddings. These embeddings capture the meaning of text based on how words appear in context.\n",
    "\n",
    "In this high-dimensional space, words or sentences with similar meanings lie close together, while unrelated ideas are farther apart.\n",
    "\n",
    "LLMs like ChatGPT use these embeddings as the foundation for understanding meaning, context, and intent -- powering applications such as chatbots, search, summarization, and many others.\n",
    "\n",
    "The quality of these embeddings largely determines how well a model understands your question and how good its responses are.\n",
    "\n",
    "Assuming that you've your `OPENAI_API_KEY` setup in the course environment, the following code will return embeddings associated with these sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.embeddings.create(\n",
    "# input=\"Machine learning is fun!\",\n",
    "# model=\"text-embedding-3-small\"\n",
    "# )\n",
    "# len(response.data[0].embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "- BoW $\\rightarrow$ simple frequency-based text representation\n",
    "\n",
    "- Word embeddings $\\rightarrow$ capture meaning and similarity\n",
    "\n",
    "- Sentence embeddings $\\rightarrow$ meaning at the sentence level\n",
    "\n",
    "- LLMs $\\rightarrow$ context-aware, dynamic embeddings at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](../../img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to large language models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language models activity \n",
    "\n",
    "Let's start with a game!\n",
    "\n",
    "Each of you will receive a sticky note with a word on it. Here's what to do:\n",
    "\n",
    "1. **Look at your word.** Don't show it to anyone!\n",
    "2. Think quickly: what word might logically follow this one?  \n",
    "    Write your predicted next word on a new sticky note.\n",
    "3. You have 20 seconds. Trust your instincts.\n",
    "4. **Pass your predicted word** to the person next to you (not the one you received).\n",
    "5. Continue until the last person in your row has written their word.\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've just created a **simple Markov model of language**  each person predicted the next word based only on limited context.\n",
    "\n",
    "> I saw the word *data* $\\rightarrow$ I wrote *science*.  \n",
    "> I saw the word *machine* $\\rightarrow$ I wrote *learning*.\n",
    "\n",
    "This is how early language models worked: predict the next word using local context and co-occurrence probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model\n",
    "\n",
    "A language model computes the probability distribution over sequences (of words or characters). Intuitively, this probability tells us how \"good\" or plausible a sequence of words is. \n",
    "\n",
    "![](../../img/voice-assistant-ex.png)\n",
    "\n",
    "Check out this [recent BMO ad](https://www.youtube.com/watch?v=VzqKtAYeJt4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"500\"\n",
       "            height=\"500\"\n",
       "            src=\"https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x3557f02f0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif\"\n",
    "\n",
    "IPython.display.IFrame(url, width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple model of language \n",
    "\n",
    "- Calculate the co-occurrence frequencies and probabilities based on these frequencies\n",
    "- Predict the next word based on these probabilities\n",
    "- This is a Markov model of language. \n",
    "\n",
    "![](../../img/Markov-bigram-probs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Markov models to meaning\n",
    "\n",
    "Markov models can predict short sequences, but they quickly fall apart with longer context.\n",
    "\n",
    "For example:\n",
    "\n",
    "> I am studying law at the University of British Columbia because I want to work as a ___\n",
    "\n",
    "To predict the last word (*lawyer*), we must remember information from the *beginning* of the sentence  something a simple Markov model cant do.\n",
    "\n",
    "> We need models that can **remember long-range dependencies** and **weigh context** differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From word prediction to transformers\n",
    "\n",
    "Earlier deep learning models like Recurrent Neural Networks (RNNs) and Long-Short Term Memory Models (LSTMs) tried to solve this by \"remembering\" previous words. But they process words one at a time, making them slow and still forgetful.\n",
    "\n",
    "**Transformer models** changed everything. They read all words **in parallel** and use **attention** to decide which words to focus on. Transformer architectures are at the heart of today's most powerful generative AI models (GPT-4, GPT-5, Gemini, LLaMA, Claude, and many others). \n",
    "\n",
    "Check out [this video on self-attention](https://www.youtube.com/watch?v=NJ_kTPwcaJU) if you want to know more. \n",
    "\n",
    "![](../../img/genai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Large Language Models (LLMs)?\n",
    "\n",
    "A **Large Language Model (LLM)** is a neural network trained to predict the next token in a sequence. \n",
    "\n",
    "By doing this billions of times across massive text corpora, the model learns:\n",
    "- grammar and syntax  \n",
    "- world knowledge  \n",
    "- relationships between concepts  \n",
    "- even reasoning patterns\n",
    "\n",
    "![](../../img/GPT-4-tech-report-abstract.png)  \n",
    "Source: [GPT-4 Technical Report (OpenAI, 2023)](https://arxiv.org/pdf/2303.08774.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common architectures\n",
    "\n",
    "|  | Decoder-only | Encoder-only | Encoder-decoder |\n",
    "|----------|---------------|---------------|-----------------|\n",
    "| **Examples** | GPT-3, LLaMA, Gemini | BERT, RoBERTa | T5, BART |\n",
    "| **Uses** | Text generation, chatbots | Text classification, embeddings | Translation, summarization |\n",
    "| **Context Handling** | Considers earlier tokens | Bidirectional (full context) | Encodes input, generates output |\n",
    "\n",
    "> Most generative models you use (ChatGPT, Claude, Gemini) are **decoder-only transformers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP pipelines before and after LLMs\n",
    "\n",
    "\n",
    "| Traditional NLP Pipeline | LLM-Powered Pipeline |\n",
    "|---------------------------|----------------------|\n",
    "| Text preprocessing, tokenization | Minimal preprocessing |\n",
    "| Feature extraction (BoW, TF-IDF, embeddings) | Implicit contextual embeddings |\n",
    "| One model per task | One model, many tasks |\n",
    "| Needs labeled data | Zero-shot and few-shot learning |\n",
    "\n",
    "\n",
    "LLMs have shifted NLP from **feature engineering** to **prompt engineering**. \n",
    "\n",
    "There are many Python libraries that make it easy to use pretrained LLMs:\n",
    "\n",
    "-  [**Transformers**](https://huggingface.co/docs/transformers/index)  unified interface for hundreds of models  \n",
    "- [**OpenAI API**](https://pypi.org/project/openai/)  GPT-3.5 / GPT-4 models  \n",
    "- [**LangChain**](https://python.langchain.com/v0.2/docs/introduction/)  building complex LLM workflows  \n",
    "- [**Haystack**](https://pypi.org/project/farm-haystack/)  retrieval-augmented generation (RAG)  \n",
    "- [**spaCy Transformers**](https://spacy.io/universe/project/spacy-transformers)  NLP with transformer backends\n",
    "engineering**.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sentiment analysis using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.995707631111145},\n",
       " {'label': 'POSITIVE', 'score': 0.9994770884513855}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "# Sentiment analysis pipeline\n",
    "analyzer = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "analyzer([\"I asked my model to predict my future, and it said '404: Life not found.'\",\n",
    "          '''Machine learning is just like cookingsometimes you follow the recipe, \n",
    "            and other times you just hope for the best!.'''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try emotion classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived',\n",
       " 'i was feeling a little vain when i did this one',\n",
       " 'i cant walk into a shop anywhere where i do not feel uncomfortable',\n",
       " 'i felt anger when at the end of a telephone call',\n",
       " 'i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia',\n",
       " 'i like to have the same breathless feeling as a reader eager to see what will happen next',\n",
       " 'i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer',\n",
       " 'i don t feel particularly agitated',\n",
       " 'i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey',\n",
       " 'i pay attention it deepens into a feeling of being invaded and helpless',\n",
       " 'i just feel extremely comfortable with the group of people that i dont even need to hide myself',\n",
       " 'i find myself in the odd position of feeling supportive of']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "exs = dataset[\"test\"][\"text\"][3:15]\n",
    "exs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline \n",
    "import torch\n",
    "\n",
    "#Load the pretrained model\n",
    "model_name = \"facebook/bart-large-mnli\"\n",
    "classifier = pipeline('zero-shot-classification', model=model_name)\n",
    "exs = dataset[\"test\"][\"text\"][:10]\n",
    "candidate_labels = [\"sadness\", \"joy\", \"love\",\"anger\", \"fear\", \"surprise\"]\n",
    "outputs = classifier(exs, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>labels</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>[sadness, anger, surprise, fear, joy, love]</td>\n",
       "      <td>[0.7367984056472778, 0.10041668266057968, 0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>[sadness, surprise, anger, fear, joy, love]</td>\n",
       "      <td>[0.7429758906364441, 0.13775911927223206, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>[love, sadness, surprise, fear, anger, joy]</td>\n",
       "      <td>[0.3153625428676605, 0.22490517795085907, 0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>[surprise, joy, love, sadness, fear, anger]</td>\n",
       "      <td>[0.4218207597732544, 0.3336693048477173, 0.217...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>[surprise, anger, fear, love, joy, sadness]</td>\n",
       "      <td>[0.5639418363571167, 0.1700027883052826, 0.086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i cant walk into a shop anywhere where i do no...</td>\n",
       "      <td>[surprise, fear, sadness, anger, joy, love]</td>\n",
       "      <td>[0.37033313512802124, 0.3655935227870941, 0.14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i felt anger when at the end of a telephone call</td>\n",
       "      <td>[anger, surprise, fear, sadness, joy, love]</td>\n",
       "      <td>[0.9760521650314331, 0.012534340843558311, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i explain why i clung to a relationship with a...</td>\n",
       "      <td>[surprise, joy, love, sadness, fear, anger]</td>\n",
       "      <td>[0.43820175528526306, 0.23223111033439636, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i like to have the same breathless feeling as ...</td>\n",
       "      <td>[surprise, joy, love, fear, anger, sadness]</td>\n",
       "      <td>[0.7675789594650269, 0.1384684145450592, 0.031...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i jest i feel grumpy tired and pre menstrual w...</td>\n",
       "      <td>[surprise, sadness, anger, fear, joy, love]</td>\n",
       "      <td>[0.7340179085731506, 0.11860340088605881, 0.07...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sequence  \\\n",
       "0  im feeling rather rotten so im not very ambiti...   \n",
       "1          im updating my blog because i feel shitty   \n",
       "2  i never make her separate from me because i do...   \n",
       "3  i left with my bouquet of red and yellow tulip...   \n",
       "4    i was feeling a little vain when i did this one   \n",
       "5  i cant walk into a shop anywhere where i do no...   \n",
       "6   i felt anger when at the end of a telephone call   \n",
       "7  i explain why i clung to a relationship with a...   \n",
       "8  i like to have the same breathless feeling as ...   \n",
       "9  i jest i feel grumpy tired and pre menstrual w...   \n",
       "\n",
       "                                        labels  \\\n",
       "0  [sadness, anger, surprise, fear, joy, love]   \n",
       "1  [sadness, surprise, anger, fear, joy, love]   \n",
       "2  [love, sadness, surprise, fear, anger, joy]   \n",
       "3  [surprise, joy, love, sadness, fear, anger]   \n",
       "4  [surprise, anger, fear, love, joy, sadness]   \n",
       "5  [surprise, fear, sadness, anger, joy, love]   \n",
       "6  [anger, surprise, fear, sadness, joy, love]   \n",
       "7  [surprise, joy, love, sadness, fear, anger]   \n",
       "8  [surprise, joy, love, fear, anger, sadness]   \n",
       "9  [surprise, sadness, anger, fear, joy, love]   \n",
       "\n",
       "                                              scores  \n",
       "0  [0.7367984056472778, 0.10041668266057968, 0.09...  \n",
       "1  [0.7429758906364441, 0.13775911927223206, 0.05...  \n",
       "2  [0.3153625428676605, 0.22490517795085907, 0.19...  \n",
       "3  [0.4218207597732544, 0.3336693048477173, 0.217...  \n",
       "4  [0.5639418363571167, 0.1700027883052826, 0.086...  \n",
       "5  [0.37033313512802124, 0.3655935227870941, 0.14...  \n",
       "6  [0.9760521650314331, 0.012534340843558311, 0.0...  \n",
       "7  [0.43820175528526306, 0.23223111033439636, 0.1...  \n",
       "8  [0.7675789594650269, 0.1384684145450592, 0.031...  \n",
       "9  [0.7340179085731506, 0.11860340088605881, 0.07...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Harms of large language models** \n",
    "\n",
    "While these models are super powerful and useful, be mindful of the harms caused by these models. Some of the harms as summarized [here]: \n",
    "\n",
    "- performance disparties\n",
    "- social biases and stereotypes\n",
    "- toxicity\n",
    "- misinformation\n",
    "- security and privacy risks\n",
    "- copyright and legal protections\n",
    "- environmental impact\n",
    "- centralization of power\n",
    "\n",
    "For more, see Stanford CS324 Lecture on [Harms of LLMs](https://stanford-cs324.github.io/winter2022/lectures/harms-1/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Takeaway message**\n",
    "\n",
    "- Language modeling began as simple next-word prediction.\n",
    "\n",
    "- Transformers introduced self-attention for contextual understanding.\n",
    "\n",
    "- LLMs scaled these ideas to billions of parameters, enabling reasoning and generation.\n",
    "\n",
    "- With great power comes great responsibility  awareness and ethical use are key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- NLP is a big and very active field. \n",
    "- We broadly explored three topics:\n",
    "    - Topic modeling \n",
    "    - Word and text representations embeddings using pretrained models \n",
    "    - Introduction to large language models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some resources if you want to get into NLP.\n",
    "\n",
    "- Check out this [CPSC course on NLP](https://www.cs.ubc.ca/~vshwartz/courses/CPSC436N-22/index.html). \n",
    "- The first resource I would recommend is the following book by Jurafsky and Martin. It's very approachable and fun. And the current edition is available online. \n",
    "    - [Speech and Language Processing by Dan Jurafsky and James H. Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "- There is a course taught at Stanford called [\"From languages to Information\"](http://web.stanford.edu/class/cs124/) by one of the co-authors of the above book, and it might be a good introduction to NLP for you. Most of the [course material](https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&disable_polymer=true) and [videos]() are available for free. \n",
    "- If you are into deep learning, you may refer to [this course](https://cs224d.stanford.edu/). Again, all lecture videos are available on youtube. \n",
    "- If you want to look at current advancements in the field, you'll find all NLP related publications [here](https://www.aclweb.org/anthology/). "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
