
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lecture 17: Introduction to natural language processing &#8212; CPSC 330 Applied Machine Learning 2025W1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/extra.css?v=6df0ab2b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/101-103-Giulia-lectures/17_natural-language-processing';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/UBC-CS-logo.png" class="logo__image only-light" alt="CPSC 330 Applied Machine Learning 2025W1 - Home"/>
    <script>document.write(`<img src="../../_static/UBC-CS-logo.png" class="logo__image only-dark" alt="CPSC 330 Applied Machine Learning 2025W1 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    UBC CPSC 330: Applied Machine Learning (2025W1)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/README.html">CPSC 330 Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../learning-objectives.html">Course Learning Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notes/01_intro.html">Lecture 1: Course Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/02_terminology-decision-trees.html">Lecture 2: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/03_ml-fundamentals.html">Lecture 3: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/04_kNNs-SVM-RBF.html">Lecture 4: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/05_preprocessing-pipelines.html">Lecture 5: Preprocessing and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/06_column-transformer-text-feats.html">Lecture 6: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and Text Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/07_linear-models.html">Lecture 7: Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/08_hyperparameter-optimization.html">Lecture 8: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/09_classification-metrics.html">Lecture 9: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/10_regression-metrics.html">Lecture 10: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/11_ensembles.html">Lecture 11: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/12_feat-importances.html">Lecture 12: Feature importances and model transparency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/13_feature-engineering-selection.html">Lecture 13: Feature engineering and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/14_K-Means.html">Lecture 14: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/15_DBSCAN-hierarchical.html">Lecture 15: More Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/16_recommender-systems.html">Lecture 16: Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/17_natural-language-processing.html">Lecture 17: Introduction to natural language processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/18_intro_to_computer-vision.html">Lecture 18: Multi-class classification and introduction to computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/19_time-series.html">Lecture 19: Time series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/20_survival-analysis.html">Lecture 20: Survival analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/21_communication.html">Lecture 21: Communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/23_deployment-conclusion.html">Lecture 23: Deployment and conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/final-exam-review-guiding-question.html">Final exam preparation: guiding questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixA.html">Appendix A: Handling class imbalance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixB.html">Appendix B: Feature Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixC.html">Appendix C: Basic text preprocessing [video]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixD.html">Appendix D: Multi-class, meta-strategies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section slides</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../102-Varada-lectures/README.html">Section 102</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-01.html">Lecture 1</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-02.html">Lecture 2</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-03.html">Lecture 3</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-04.html">Lecture 4</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-05.html">Lecture 5</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-06.html">Lecture 6</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-07.html">Lecture 7</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-08.html">Lecture 8</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-09.html">Lecture 9</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-10.html">Lecture 10</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-11.html">Lecture 11</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-12.html">Lecture 12</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-13.html">Lecture 13</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-14.html">Lecture 14</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-15.html">Lecture 15</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-16.html">Lecture 16</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-17.html">Lecture 17</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-18.html">Lecture 18</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-19.html">Lecture 19</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-20.html">Lecture 20</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-21.html">Lecture 21</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-23.html">Lecture 23</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-CS/cpsc330-2025W1" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/101-103-Giulia-lectures/17_natural-language-processing.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 17: Introduction to natural language processing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-motivation">Topic modeling motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-social-media">LDA topics in social media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-toy-example">Topic modeling toy example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-pipeline">Topic modeling pipeline</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-representations-and-word-embeddings">Text representations and word embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-hypothesis">Distributional hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings-the-idea">Word embeddings: The idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-similarity-between-vectors">Measuring similarity between vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-question">Discussion question</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-word-embeddings">Using pre-trained word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-words-sentence-embeddings">Beyond words: sentence embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">Introduction to large language models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-models-activity">Language models activity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-model">Language model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-model-of-language">A simple model of language</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-markov-models-to-meaning">From Markov models to meaning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-word-prediction-to-transformers">From word prediction to transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-large-language-models-llms">What are Large Language Models (LLMs)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-architectures">Common architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-pipelines-before-and-after-llms">NLP pipelines before and after LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sentiment-analysis-using-a-pretrained-model">Example: Sentiment analysis using a pretrained model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><img alt="" src="../../_images/330-banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-17-introduction-to-natural-language-processing">
<h1>Lecture 17: Introduction to natural language processing<a class="headerlink" href="#lecture-17-introduction-to-natural-language-processing" title="Link to this heading">#</a></h1>
<p>UBC 2025-26</p>
<section id="imports">
<h2>Imports<a class="headerlink" href="#imports" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">string</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">plotting_functions_unsup</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">IPython</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">npr</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">comat</span><span class="w"> </span><span class="kn">import</span> <span class="n">CooccurrenceMatrix</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">nltk.tokenize</span><span class="w"> </span><span class="kn">import</span> <span class="n">sent_tokenize</span><span class="p">,</span> <span class="n">word_tokenize</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MyPreprocessor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">DATA_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">),</span> <span class="s2">&quot;data/&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7edc2029a1d3810ecfb1576c9dc0ecd09822171a025182f68fad9c1321c88082.png" src="../../_images/7edc2029a1d3810ecfb1576c9dc0ecd09822171a025182f68fad9c1321c88082.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\Giulia\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p><br><br></p>
</section>
<section id="learning-objectives">
<h2>Learning objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<p>By the end of this lecture, you will be able to:</p>
<ul class="simple">
<li><p>Differentiate between common similarity metrics, including Euclidean distance, dot product similarity, and cosine similarity.</p></li>
<li><p>Explain what Natural Language Processing (NLP) is and identify common applications.</p></li>
<li><p>Describe the idea behind topic modeling and apply it to uncover hidden structure and meaning in text data.</p></li>
<li><p>Distinguish between topic modeling and clustering in terms of goals and outputs.</p></li>
<li><p>Describe key text representations, from bag-of-words to word and sentence embeddings, and explain why richer representations are needed.</p></li>
<li><p>Use word embeddings to explore similarities and analogies between words.</p></li>
<li><p>Explain the difference between word embeddings and sentence embeddings and when each is useful.</p></li>
<li><p>Interpret similarities between words, sentences, or documents using cosine similarity.</p></li>
<li><p>Explain what a language model is and how it predicts or generates text.</p></li>
<li><p>Summarize the main ideas behind large language models (LLMs) and their role in modern NLP.</p></li>
<li><p>Recognize potential biases and ethical concerns when using pretrained language models.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="what-is-natural-language-processing-nlp">
<h2>What is Natural Language Processing (NLP)?<a class="headerlink" href="#what-is-natural-language-processing-nlp" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>What should a search engine return when asked the following question?</p></li>
</ul>
<p><img alt="" src="../../_images/lexical_ambiguity.png" /></p>
<!-- <center> -->
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> -->
<!-- </center> --><p><img alt="" src="../../_images/Google_search.png" /></p>
<!-- <center> -->
<!-- <img src="img/Google_search.png" width="900" height="900"> -->
<!-- </center> --><p>Natural Language Processing (NLP) is a branch of machine learning focused on enabling computers to understand, interpret, and generate human language.</p>
<p><img alt="" src="../../_images/WhatisNLP.png" /></p>
<!-- <center> -->
<!-- <img src="img/WhatisNLP.png" width="800" height="800"> -->
<!-- </center> -->    <p><strong>Everyday NLP applications</strong></p>
<p><img alt="" src="../../_images/annotation-image.png" /></p>
<!-- <center> -->
<!-- <img src="img/annotation-image.png" height="1200" width="1200"> -->
<!-- </center> --><p>And of course, general-purpose conversational chatbots</p>
<ul class="simple">
<li><p>ChatGPT (by OpenAI)</p></li>
<li><p>Claude (by Anthropic)</p></li>
<li><p>Gemini (by Google DeepMind; formerly Bard)</p></li>
<li><p>Copilot (by Microsoft, integrated into Office and Windows)</p></li>
<li><p>Perplexity AI (search-augmented conversational assistant)</p></li>
<li><p>…</p></li>
</ul>
<p><strong>Why is NLP hard?</strong></p>
<ul class="simple">
<li><p>Language is complex and subtle.</p></li>
<li><p>Language is ambiguous at different levels.</p></li>
<li><p>Language understanding involves common-sense knowledge and real-world reasoning.</p></li>
<li><p>All the problems related to representation and reasoning in artificial intelligence arise in this domain.</p></li>
</ul>
<p><strong>Exmaple: Lexical ambiguity</strong></p>
<p><br><br></p>
<p><img alt="" src="../../_images/lexical_ambiguity.png" /></p>
<!-- <img src="img/lexical_ambiguity.png" width="1000" height="1000"> -->
<p><strong>Example: Referential ambiguity</strong>
<br><br></p>
<!-- <img src="img/referential_ambiguity.png" width="1000" height="1000"> -->
<p><img alt="" src="../../_images/referential_ambiguity.png" /></p>
<p><strong>Example: <a class="reference external" href="http://www.fun-with-words.com/ambiguous_headlines.html">Ambiguous news headlines</a></strong></p>
<blockquote>
PROSTITUTES APPEAL TO POPE
</blockquote>    
<ul class="simple">
<li><p><strong>appeal to</strong> means make a serious or urgent request or be attractive or interesting?</p></li>
</ul>
<blockquote>
KICKING BABY CONSIDERED TO BE HEALTHY    
</blockquote> 
<ul class="simple">
<li><p><strong>kicking</strong> is used as an adjective or a verb?</p></li>
</ul>
<blockquote>
MILK DRINKERS ARE TURNING TO POWDER
</blockquote>
<ul class="simple">
<li><p><strong>turning</strong> means becoming or take up?</p></li>
</ul>
<p><strong>NLP in industry</strong></p>
<p>NLP powers a wide range of real-world applications across industries by enabling machines to understand and generate human language.</p>
<ul class="simple">
<li><p>customer service (chatbots, voice assistants)</p></li>
<li><p>marketing (sentiment and trend analysis)</p></li>
<li><p>finance (document summarization, fraud detection)</p></li>
<li><p>healthcare (clinical text analysis, medical coding)</p></li>
<li><p>law (contract review, information extraction)</p></li>
<li><p>…</p></li>
</ul>
<p><strong>Overall goal of this lecture</strong></p>
<ul class="simple">
<li><p>Give you a quick introduction to you of this important field in artificial intelligence which extensively used machine learning.</p></li>
<li><p>This is a huge field. We’ll focus on the following three topics which are likely to be useful for you.</p>
<ul>
<li><p>Topic modeling</p></li>
<li><p>Word and text representations</p></li>
<li><p>Quick introduction to LLMs</p></li>
</ul>
</li>
</ul>
<p><br><br></p>
</section>
<section id="topic-modeling">
<h2>Topic modeling<a class="headerlink" href="#topic-modeling" title="Link to this heading">#</a></h2>
<p><strong>Topic modeling introduction activity (~5 mins)</strong></p>
<ul class="simple">
<li><p>Consider the following documents.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;toy_clustering.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>elegant fashion model</td>
    </tr>
    <tr>
      <th>2</th>
      <td>fashion model at famous probabilistic topic mo...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>fresh elegant fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>famous elegant fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>probabilistic conference</td>
    </tr>
    <tr>
      <th>6</th>
      <td>creative probabilistic model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>model diet apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>8</th>
      <td>probabilistic model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>10</th>
      <td>fresh apple kiwi health diet</td>
    </tr>
    <tr>
      <th>11</th>
      <td>health nutrition</td>
    </tr>
    <tr>
      <th>12</th>
      <td>fresh apple kiwi juice nutrition</td>
    </tr>
    <tr>
      <th>13</th>
      <td>probabilistic topic model conference</td>
    </tr>
    <tr>
      <th>14</th>
      <td>probabilistic topi model</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Discuss the following questions with your neighbour</strong></p>
<ol class="arabic simple">
<li><p>Suppose you are asked to cluster these documents manually. How many clusters would you identify?</p></li>
<li><p>What are the prominent words in each cluster?</p></li>
<li><p>Are there documents which could be assigne to more than one cluster?</p></li>
</ol>
<section id="topic-modeling-motivation">
<h3>Topic modeling motivation<a class="headerlink" href="#topic-modeling-motivation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Humans are pretty good at reading and understanding a document and answering questions such as</p>
<ul>
<li><p>What is it about?</p></li>
<li><p>Which documents is it related to?</p></li>
</ul>
</li>
<li><p>What if you’re given a large collection of documents on a variety of topics.</p></li>
</ul>
<p><strong>A corpus of news articles</strong></p>
<p><img alt="" src="../../_images/TM_NYT_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_NYT_articles.png" height="2000" width="2000">  -->
<!-- </center> --><p><strong>Example: A corpus of food magazines</strong></p>
<p><img alt="" src="../../_images/TM_food_magazines.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_food_magazines.png" height="2000" width="2000">  -->
<!-- </center> --><p><strong>A corpus of scientific articles</strong></p>
<p><img alt="" src="../../_images/TM_science_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_science_articles.png" height="2000" width="2000">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">Dave Blei’s presentation</a>)</p>
<ul class="simple">
<li><p>It would take years to read all documents and organize and categorize them so that they are easy to search.</p></li>
<li><p>You need an automated way</p>
<ul>
<li><p>to get an idea of what’s going on in the data or</p></li>
<li><p>to pull documents related to a certain topic</p></li>
</ul>
</li>
<li><p><strong>Topic modeling</strong> gives you an ability to summarize the major themes in a large collection of documents (corpus).</p>
<ul>
<li><p>Example: The major themes in a collection of news articles could be</p>
<ul>
<li><p><strong>politics</strong></p></li>
<li><p><strong>entertainment</strong></p></li>
<li><p><strong>sports</strong></p></li>
<li><p><strong>technology</strong></p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Topic modeling is a great EDA tool to get a sense of what’s going on in a large corpus.</p></li>
<li><p>Some examples</p>
<ul>
<li><p>If you want to pull documents related to a particular lawsuit.</p></li>
<li><p>You want to examine people’s sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.</p></li>
</ul>
</li>
</ul>
<p><strong>How do you do topic modeling?</strong></p>
<ul class="simple">
<li><p>A common tool to solve such problems is <strong>unsupervised ML methods</strong>.</p></li>
<li><p>Given the hyperparameter <span class="math notranslate nohighlight">\(K\)</span>, the goal of topic modeling is to describe a set of documents using <span class="math notranslate nohighlight">\(K\)</span> “topics”.</p></li>
<li><p>In unsupervised setting, the input of topic modeling is</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>and the output is</p>
<ol class="arabic simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
</ul>
</li>
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ol>
</li>
</ul>
<p><strong>Topic modeling: Example</strong></p>
<ul class="simple">
<li><p>Topic-words association</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>A topic is a mixture of words.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_word_topics.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_word_topics.png" height="1000" width="1000">  -->
<!-- </center> -->    <p><strong>Topic modeling: Example</strong></p>
<ul class="simple">
<li><p>Document-topics association</p>
<ul>
<li><p>For each document, what topics are expressed by the document?</p></li>
<li><p>A document is a mixture of topics.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_doc_topics.png" /></p>
<!-- <center> -->    
<!-- <img src="img/topic_modeling_doc_topics.png" height="800" width="800">  -->
<!-- </center> -->    <p><strong>Topic modeling: Input and output</strong></p>
<ul class="simple">
<li><p>Input</p>
<ul>
<li><p>A large collection of documents</p></li>
<li><p>A value for the hyperparameter <span class="math notranslate nohighlight">\(K\)</span> (e.g., <span class="math notranslate nohighlight">\(K = 3\)</span>)</p></li>
</ul>
</li>
<li><p>Output</p>
<ul>
<li><p>For each topic, what words describe that topic?</p></li>
<li><p>For each document, what topics are expressed by the document?</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/topic_modeling_output.png" /></p>
<!-- <center> -->
<!-- <img src="img/topic_modeling_output.png" height="800" width="800">  -->
<!-- </center> -->    <p><strong>Topic modeling: Some applications</strong></p>
<ul class="simple">
<li><p>Topic modeling is a great EDA tool to get a sense of what’s going on in a large corpus.</p></li>
<li><p>Some examples</p>
<ul>
<li><p>If you want to pull documents related to a particular lawsuit.</p></li>
<li><p>You want to examine people’s sentiment towards a particular candidate and/or political party and so you want to pull tweets or Facebook posts related to election.</p></li>
</ul>
</li>
</ul>
<p><strong>Topic modeling examples</strong></p>
<p><strong>Topic modeling: Input</strong></p>
<p><img alt="" src="../../_images/TM_science_articles.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_science_articles.png" height="2000" width="2000">  -->
<!-- </center>     -->
<p>Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a></p>
<p><strong>Topic modeling: output</strong></p>
<p><img alt="" src="../../_images/TM_topics.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_topics.png" height="900" width="900">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a>)</p>
<p><strong>Topic modeling: output with interpretation</strong></p>
<ul class="simple">
<li><p>Assigning labels is a human thing.</p></li>
</ul>
<p><img alt="" src="../../_images/TM_topics_with_labels.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_topics_with_labels.png" height="800" width="800">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/talks/Blei_Science_2008.pdf">David Blei’s presentation</a>)</p>
<p><strong>LDA (Latent Dirichlet Allocation) topics in Yale Law Journal</strong></p>
<p><img alt="" src="../../_images/TM_yale_law_journal.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_yale_law_journal.png" height="1500" width="1500">  -->
<!-- </center>     -->
<p>(Credit: <a class="reference external" href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">David Blei’s paper</a>)</p>
</section>
<section id="lda-topics-in-social-media">
<h3>LDA topics in social media<a class="headerlink" href="#lda-topics-in-social-media" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/TM_health_topics_social_media.png" /></p>
<!-- <center> -->
<!-- <img src="img/TM_health_topics_social_media.png" height="1300" width="1300">  -->
<!-- </center> -->
<p>(Credit: <a class="reference external" href="https://journals.plos.org/plosone/article/figure?id=10.1371/journal.pone.0103408.g002">Health topics in social media</a>)</p>
<p>In this lecture, I will demonstrate how to perform topic modeling using the <strong>Latent Dirichlet Allocation</strong> model implemented in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>. We won’t delve into the inner workings of the model, as it falls outside the scope of this course. Instead, our objective is to understand how to apply it to your specific problems and comprehend the model’s input and output.</p>
</section>
<section id="topic-modeling-toy-example">
<h3>Topic modeling toy example<a class="headerlink" href="#topic-modeling-toy-example" title="Link to this heading">#</a></h3>
<p>Let’s work with a toy example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;toy_lda_data.csv&quot;</span><span class="p">)</span>
<span class="n">toy_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>doc_id</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>fashion model pattern</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>fresh fashion model</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>6</th>
      <td>7</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>7</th>
      <td>8</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>8</th>
      <td>9</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>9</th>
      <td>10</td>
      <td>creative fashion model</td>
    </tr>
    <tr>
      <th>10</th>
      <td>11</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>11</th>
      <td>12</td>
      <td>famous fashion model</td>
    </tr>
    <tr>
      <th>12</th>
      <td>13</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>13</th>
      <td>14</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>14</th>
      <td>15</td>
      <td>probabilistic model pattern</td>
    </tr>
    <tr>
      <th>15</th>
      <td>16</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>16</th>
      <td>17</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>17</th>
      <td>18</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>18</th>
      <td>19</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>19</th>
      <td>20</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>20</th>
      <td>21</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>21</th>
      <td>22</td>
      <td>fashion model probabilistic topic model confer...</td>
    </tr>
    <tr>
      <th>22</th>
      <td>23</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>23</th>
      <td>24</td>
      <td>kiwi health nutrition</td>
    </tr>
    <tr>
      <th>24</th>
      <td>25</td>
      <td>fresh apple health</td>
    </tr>
    <tr>
      <th>25</th>
      <td>26</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>26</th>
      <td>27</td>
      <td>creative health nutrition</td>
    </tr>
    <tr>
      <th>27</th>
      <td>28</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>28</th>
      <td>29</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>29</th>
      <td>30</td>
      <td>hidden markov model probabilistic</td>
    </tr>
    <tr>
      <th>30</th>
      <td>31</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>31</th>
      <td>32</td>
      <td>probabilistic topic model</td>
    </tr>
    <tr>
      <th>32</th>
      <td>33</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>33</th>
      <td>34</td>
      <td>apple kiwi health</td>
    </tr>
    <tr>
      <th>34</th>
      <td>35</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>35</th>
      <td>36</td>
      <td>fresh kiwi health</td>
    </tr>
    <tr>
      <th>36</th>
      <td>37</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>37</th>
      <td>38</td>
      <td>apple kiwi nutrition</td>
    </tr>
    <tr>
      <th>38</th>
      <td>39</td>
      <td>apple kiwi nutrition</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Input to the LDA topic model is bag-of-words representation of text.</p></li>
<li><p>Let’s create bag-of-words representation of “text” column.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">toy_X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">toy_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="n">toy_X</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;39x15 sparse matrix of type &#39;&lt;class &#39;numpy.int64&#39;&gt;&#39;
	with 124 stored elements in Compressed Sparse Row format&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span> <span class="c1"># vocabulary</span>
<span class="n">vocab</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([&#39;apple&#39;, &#39;conference&#39;, &#39;creative&#39;, &#39;famous&#39;, &#39;fashion&#39;, &#39;fresh&#39;,
       &#39;health&#39;, &#39;hidden&#39;, &#39;kiwi&#39;, &#39;markov&#39;, &#39;model&#39;, &#39;nutrition&#39;,
       &#39;pattern&#39;, &#39;probabilistic&#39;, &#39;topic&#39;], dtype=object)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15
</pre></div>
</div>
</div>
</div>
<p>Let’s try to create a topic model with sklearn’s <code class="docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># number of topics</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">toy_X</span><span class="p">)</span> 
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">toy_X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Once we have a fitted model we can get the word-topic association and document-topic association</p></li>
<li><p>Word-topic association</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">lda.components_</span></code> gives us the weights associated with each word for each topic. In other words, it tells us which word is important for which topic.</p></li>
</ul>
</li>
<li><p>Document-topic association</p>
<ul>
<li><p>Calling transform on the data gives us document-topic association.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.33380754,  3.31038074,  0.33476534,  0.33397112,  0.36695134,
         0.33439238,  0.33381373,  0.35771821,  0.33380649,  0.35771821,
        17.78521263,  0.33380761,  0.3573886 , 17.31634363, 15.32791718],
       [ 8.33224516,  0.33400489,  2.2173627 ,  0.33411086,  0.33732465,
         3.28753559,  5.33223002,  0.33435326,  9.33224759,  0.33435326,
         0.33797555,  8.3322447 ,  0.33462759,  0.33440682,  0.33425967],
       [ 0.3339473 ,  0.35561437,  0.44787197,  8.33191802, 14.29572402,
         0.37807203,  0.33395626,  1.30792853,  0.33394593,  1.30792853,
        13.87681182,  0.33394769,  2.30798381,  0.34924955,  0.33782315]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 15)
</pre></div>
</div>
</div>
</div>
<p>Let’s look at the words with highest weights for each topic more systematically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
model         kiwi          fashion       
probabilistic apple         model         
topic         nutrition     famous        
conference    health        pattern       
fashion       fresh         hidden        
markov        creative      markov        
hidden        model         creative      
pattern       fashion       fresh         
creative      pattern       conference    
fresh         probabilistic probabilistic 
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Here is how we can interpret the topics</p>
<ul>
<li><p>Topic 0 <span class="math notranslate nohighlight">\(\rightarrow\)</span> ML modeling</p></li>
<li><p>Topic 1 <span class="math notranslate nohighlight">\(\rightarrow\)</span> fruit and nutrition</p></li>
<li><p>Topic 2 <span class="math notranslate nohighlight">\(\rightarrow\)</span> fashion</p></li>
</ul>
</li>
</ul>
<p>Let’s look at distribution of topics for a document</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">toy_df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;famous fashion model&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">document_topics</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0.08791477, 0.08338644, 0.82869879])
</pre></div>
</div>
</div>
</div>
<p>This document is made up of</p>
<ul class="simple">
<li><p>~83% topic 2</p></li>
<li><p>~9% topic 0</p></li>
<li><p>~8% topic 1.</p></li>
</ul>
</section>
<section id="topic-modeling-pipeline">
<h3>Topic modeling pipeline<a class="headerlink" href="#topic-modeling-pipeline" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Above we worked with toy data. In the real world, we usually need to preprocess the data before passing it to LDA.</p></li>
<li><p>Here are typical steps if you want to carry out topic modeling on real-world data.</p>
<ul>
<li><p>Preprocess your corpus.</p></li>
<li><p>Train LDA.</p></li>
<li><p>Interpret your topics.</p></li>
</ul>
</li>
</ul>
<p><strong>Data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">wikipedia</span>

<span class="n">queries</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Artificial Intelligence&quot;</span><span class="p">,</span>
    <span class="s2">&quot;unsupervised learning&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Supreme Court of Canada&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Peace, Order, and Good Government&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Canadian constitutional law&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ice hockey&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">wiki_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;wiki query&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="p">[]}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">queries</span><span class="p">)):</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">page</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="n">wiki_dict</span><span class="p">[</span><span class="s2">&quot;wiki query&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">queries</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">wiki_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">wiki_dict</span><span class="p">)</span>
<span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) is the capability...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>In machine learning, supervised learning (SL) ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey (or simply hockey in North America)...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Preprocessing the corpus</strong></p>
<ul class="simple">
<li><p><strong>Preprocessing is crucial!</strong></p></li>
<li><p>Tokenization, converting text to lower case</p></li>
<li><p>Removing punctuation and stopwords</p></li>
<li><p>Discarding words with length &lt; threshold or word frequency &lt; threshold</p></li>
<li><p>Possibly lemmatization: Consider the lemmas instead of inflected forms.</p></li>
<li><p>Depending upon your application, restrict to specific part of speech;</p>
<ul>
<li><p>For example, only consider nouns, verbs, and adjectives</p></li>
</ul>
</li>
</ul>
<p>Check out <a class="reference internal" href="#AppendixC.ipynb"><span class="xref myst">AppendixC</span></a> for basic preprocessing in NLP.</p>
<p>We’ll use <a class="reference external" href="https://spacy.io/"><code class="docutils literal notranslate"><span class="pre">spaCy</span></code></a> for preprocessing. Check out available token attributes <a class="reference external" href="https://spacy.io/usage/rule-based-matching#adding-patterns-attributes">here</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">spacy</span>

<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;parser&quot;</span><span class="p">,</span> <span class="s2">&quot;ner&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">preprocess</span><span class="p">(</span>
    <span class="n">doc</span><span class="p">,</span>
    <span class="n">min_token_len</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">irrelevant_pos</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ADV&quot;</span><span class="p">,</span> <span class="s2">&quot;PRON&quot;</span><span class="p">,</span> <span class="s2">&quot;CCONJ&quot;</span><span class="p">,</span> <span class="s2">&quot;PUNCT&quot;</span><span class="p">,</span> <span class="s2">&quot;PART&quot;</span><span class="p">,</span> <span class="s2">&quot;DET&quot;</span><span class="p">,</span> <span class="s2">&quot;ADP&quot;</span><span class="p">,</span> <span class="s2">&quot;SPACE&quot;</span><span class="p">],</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text</span>
<span class="sd">    and return a preprocessed string.</span>

<span class="sd">    Parameters</span>
<span class="sd">    -------------</span>
<span class="sd">    doc : (spaCy doc object)</span>
<span class="sd">        the spacy doc object of the text</span>
<span class="sd">    min_token_len : (int)</span>
<span class="sd">        min_token_length required</span>
<span class="sd">    irrelevant_pos : (list)</span>
<span class="sd">        a list of irrelevant pos tags</span>

<span class="sd">    Returns</span>
<span class="sd">    -------------</span>
<span class="sd">    (str) the preprocessed text</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">clean_text</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">token</span><span class="o">.</span><span class="n">is_stop</span> <span class="o">==</span> <span class="kc">False</span>  <span class="c1"># Check if it&#39;s not a stopword</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">token</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">min_token_len</span>  <span class="c1"># Check if the word meets minimum threshold</span>
            <span class="ow">and</span> <span class="n">token</span><span class="o">.</span><span class="n">pos_</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">irrelevant_pos</span>
        <span class="p">):</span>  <span class="c1"># Check if the POS is in the acceptable POS tags</span>
            <span class="n">lemma</span> <span class="o">=</span> <span class="n">token</span><span class="o">.</span><span class="n">lemma_</span>  <span class="c1"># Take the lemma of the word</span>
            <span class="n">clean_text</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lemma</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="k">return</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">preprocess</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">nlp</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">wiki_df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>wiki query</th>
      <th>text</th>
      <th>text_pp</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Artificial Intelligence</td>
      <td>Artificial intelligence (AI) is the capability...</td>
      <td>artificial intelligence capability computation...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>unsupervised learning</td>
      <td>In machine learning, supervised learning (SL) ...</td>
      <td>machine learning supervised learning type mach...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Supreme Court of Canada</td>
      <td>The Supreme Court of Canada (SCC; French: Cour...</td>
      <td>supreme court canada scc french cour suprême c...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Peace, Order, and Good Government</td>
      <td>In many Commonwealth jurisdictions, the phrase...</td>
      <td>commonwealth jurisdiction phrase peace order g...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Canadian constitutional law</td>
      <td>Canadian constitutional law (French: droit con...</td>
      <td>canadian constitutional law french droit const...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>ice hockey</td>
      <td>Ice hockey (or simply hockey in North America)...</td>
      <td>ice hockey hockey north america team sport pla...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>

<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">wiki_df</span><span class="p">[</span><span class="s2">&quot;text_pp&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">LatentDirichletAllocation</span>

<span class="n">n_topics</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LatentDirichletAllocation</span><span class="p">(</span>
    <span class="n">n_components</span><span class="o">=</span><span class="n">n_topics</span><span class="p">,</span> <span class="n">learning_method</span><span class="o">=</span><span class="s2">&quot;batch&quot;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="n">document_topics</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lda.components_.shape: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>lda.components_.shape: (3, 4095)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sorting</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">lda</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">tools</span><span class="o">.</span><span class="n">print_topics</span><span class="p">(</span>
    <span class="n">topics</span><span class="o">=</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">feature_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">sorting</span><span class="o">=</span><span class="n">sorting</span><span class="p">,</span>
    <span class="n">topics_per_chunk</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">n_words</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>topic 0       topic 1       topic 2       
--------      --------      --------      
displaystyle  court         hockey        
algorithm     intelligence  player        
learning      problem       team          
law           artificial    ice           
function      machine       league        
training      human         play          
learn         use           puck          
provincial    decision      game          
court         include       penalty       
federal       learning      canada        
</pre></div>
</div>
</div>
</div>
<p>Check out some recent topic modeling tools</p>
<ul class="simple">
<li><p><a class="reference external" href="https://top2vec.readthedocs.io/en/stable/Top2Vec.html">Topic2Vec</a></p></li>
<li><p><a class="reference external" href="https://maartengr.github.io/BERTopic/index.html">BERTopic</a></p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="text-representations-and-word-embeddings">
<h2>Text representations and word embeddings<a class="headerlink" href="#text-representations-and-word-embeddings" title="Link to this heading">#</a></h2>
<section id="motivation">
<h3>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h3>
<p>Do large language models such as ChatGPT <em>understand</em> your questions to some extent and provide useful responses?</p>
<p>What would it take for a machine to “understand” language?<br />
A first step is to find a way to represent text, numbers that capture meaning.</p>
<p>So far, we have seen <strong>bag-of-words (BoW)</strong> representation using <code class="docutils literal notranslate"><span class="pre">CountVectorizer</span></code> in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<p>Let’s quickly recall what that looks like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_extraction.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">docs</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;This movie is amazing&quot;</span><span class="p">,</span> <span class="s2">&quot;This movie is terrible&quot;</span><span class="p">]</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">vec</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>amazing</th>
      <th>is</th>
      <th>movie</th>
      <th>terrible</th>
      <th>this</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What are some limitations of Bag-of-Words?</p>
<ul class="simple">
<li><p>Sparse, high-dimensional vectors</p></li>
<li><p>Only capture word frequency</p></li>
<li><p>Ignore word order and context</p></li>
<li><p>Do not put similar words (e.g., happy, joyful) close together</p></li>
</ul>
<p>BoW represents documents, but it treats each word as an independent token,
there’s no notion of word meaning or relationships between words.</p>
<p>In this part of the lecture, we are going to go one step back and talk about word representations. Why care about word representations?</p>
<ul class="simple">
<li><p>Words are the basic building blocks of language; the smallest units that carry meaning.</p></li>
<li><p>To truly understand a document, a model must first understand the meaning of the words in it.</p></li>
<li><p>If we can represent each word in a way that captures its meaning,
then we can combine these representations to understand larger pieces of text (sentences, paragraphs, documents).</p></li>
<li><p>In other words, to represent text meaningfully, we must start with word meaning.</p></li>
</ul>
<p>This brings us to a key question: How can we represent the meaning of individual words using numbers?</p>
<p><br><br></p>
</section>
<section id="distributional-hypothesis">
<h3>Distributional hypothesis<a class="headerlink" href="#distributional-hypothesis" title="Link to this heading">#</a></h3>
<p><strong>Activity: Context and word meaning</strong></p>
<p>Pair up with a neighbor and try to guess the meanings of the following made-up words: flibbertigibbet and groak.</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>The plot twist was totally unexpected, making it a <strong>flibbertigibbet</strong> experience.</p></li>
<li><p>Despite its <strong>groak</strong> special effects, the storyline captivated my attention till the end.</p></li>
<li><p>I found the character development rather <strong>groak</strong>, failing to evoke empathy.</p></li>
<li><p>The cinematography is <strong>flibbertigibbet</strong>, showcasing breathtaking landscapes.</p></li>
<li><p>Sadly, the movie’s potential was overshadowed by its <strong>groak</strong> pacing.</p></li>
</ol>
</div></blockquote>
<p>Discussion:</p>
<ul class="simple">
<li><p>How did you infer the meanings of these words?</p></li>
<li><p>Which words or phrases helped you?</p></li>
</ul>
<p><br><br><br><br></p>
<ul class="simple">
<li><p>In the previous activity, you guessed the meaning of flibbertigibbet and groak based on surrounding words.</p></li>
<li><p>That’s exactly what machines do when they learn from text.</p></li>
<li><p>This idea is called <strong>distributional hypothesis</strong>.</p></li>
</ul>
<blockquote> 
    <p>You shall know a word by the company it keeps.</p>
    <footer>Firth, 1957</footer>        
</blockquote>
<p>In other words, words that appear in similar contexts tend to have similar meanings.</p>
</section>
<section id="word-embeddings-the-idea">
<h3>Word embeddings: The idea<a class="headerlink" href="#word-embeddings-the-idea" title="Link to this heading">#</a></h3>
<p>Building on this idea, modern NLP systems learn word embeddings, dense vector representations of words that capture these contextual relationships.</p>
<p><img alt="" src="../../_images/t-SNE_word_embeddings.png" /></p>
<p>(Attribution: <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">Jurafsky and Martin 3rd edition</a>)</p>
<p>Example:</p>
<blockquote>
<div><p>“The plot twist was totally unexpected, making it a <strong>flibbertigibbet</strong> experience.”</p>
</div></blockquote>
<blockquote>
<div><p>“The plot twist was totally unexpected, making it a <strong>delightful</strong> experience.”</p>
</div></blockquote>
<p>The goal: words like <strong>flibbertigibbet</strong> and <strong>delightful</strong> should be close in the embedding space.</p>
<p>Word embeddings are built on the distributional hypothesis. They mathematically capture the idea that context defines meaning.</p>
<p><br><br></p>
</section>
<section id="measuring-similarity-between-vectors">
<h3>Measuring similarity between vectors<a class="headerlink" href="#measuring-similarity-between-vectors" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>To create a vector space where similar words are close together, we need some metric to measure distances between representation.</p></li>
<li><p>We have used the Euclidean distance before for numeric features.</p></li>
<li><p>For sparse features, the most commonly used metrics are Dot product and Cosine distance.</p></li>
<li><p>Let’s look at an example.</p></li>
</ul>
<p>Euclidean distance</p>
<div class="math notranslate nohighlight">
\[distance(vec1, vec2) = \sqrt{\sum_{i =1}^{n} (vec1_i - vec2_i)^2}\]</div>
<p>Dot product similarity: $<span class="math notranslate nohighlight">\(similarity_{dot product}(vec1,vec2) = vec1.vec2\)</span>$</p>
<p>Cosine similarity: normalized version of dot product.
$<span class="math notranslate nohighlight">\(similarity_{cosine}(vec1,vec2) = \frac{vec1.vec2}{\left\lVert vec1\right\rVert_2 \left\lVert vec2\right\rVert_2}\)</span>$</p>
<p>Where,</p>
<ul class="simple">
<li><p>The L2 norm of <span class="math notranslate nohighlight">\(vec1\)</span> is the magnitude of <span class="math notranslate nohighlight">\(vec1\)</span>
$<span class="math notranslate nohighlight">\(\left\lVert vec1\right\rVert_2 = \sqrt{\sum_i vec1_i^2}\)</span>$</p></li>
<li><p>The L2 norm of <span class="math notranslate nohighlight">\(vec2\)</span> is the magnitude of <span class="math notranslate nohighlight">\(vec2\)</span>
$<span class="math notranslate nohighlight">\(\left\lVert vec2\right\rVert_2 = \sqrt{\sum_i vec2_i^2}\)</span>$</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">dot</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy.linalg</span><span class="w"> </span><span class="kn">import</span> <span class="n">norm</span>

<span class="c1"># Euclidean distance: smaller distance --&gt; more similar</span>
<span class="k">def</span><span class="w"> </span><span class="nf">euclidean_distance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
    
<span class="c1"># Dot product similarity: larger dot product --&gt; more similar</span>
<span class="k">def</span><span class="w"> </span><span class="nf">dot_product_similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Cosine similarity: larger similarity score --&gt; more similar</span>
<span class="k">def</span><span class="w"> </span><span class="nf">cosine_similarity</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">norm</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">v1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Euclidean Distance: </span><span class="si">{</span><span class="n">euclidean_distance</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Dot Product Similarity: </span><span class="si">{</span><span class="n">dot_product_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cosine Similarity: </span><span class="si">{</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span><span class="w"> </span><span class="n">v2</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Euclidean Distance: 5.1962
Dot Product Similarity: 14.0000
Cosine Similarity: 0.5098
</pre></div>
</div>
</div>
</div>
</section>
<section id="discussion-question">
<h3>Discussion question<a class="headerlink" href="#discussion-question" title="Link to this heading">#</a></h3>
<p>Suppose you are recommending items based on similarity between items. Given a query vector “Query” in the picture below and the three item vectors, determine the ranking for the three similarity measures below:</p>
<ul class="simple">
<li><p>Similarity based on Euclidean distance</p></li>
<li><p>similarity based on dot product</p></li>
<li><p>Cosine similarity</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/distance-metrics.png"><img alt="" src="../../_images/distance-metrics.png" style="width: 500px; height: 800px;" />
</a>
<!-- ![](../img/distance-metrics.png) -->
<ul class="simple">
<li><p>Adapted from <a class="reference external" href="https://developers.google.com/machine-learning/recommendation/overview/candidate-generation">here</a>.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
<section id="using-pre-trained-word-embeddings">
<h3>Using pre-trained word embeddings<a class="headerlink" href="#using-pre-trained-word-embeddings" title="Link to this heading">#</a></h3>
<p>Creating these representations on your own is resource intensive. So people typically use “pretrained” embeddings.
A number of pre-trained word embeddings are available. The most popular ones are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://code.google.com/archive/p/word2vec/">word2vec</a></p>
<ul>
<li><p>trained on several corpora using the word2vec algorithm</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">wikipedia2vec</a></p>
<ul>
<li><p>pretrained embeddings for 12 languages</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe</a></p>
<ul>
<li><p>trained using <a class="reference external" href="https://nlp.stanford.edu/pubs/glove.pdf">the GloVe algorithm</a></p></li>
<li><p>published by Stanford University</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://fasttext.cc/docs/en/pretrained-vectors.html">fastText pre-trained embeddings for 294 languages</a></p>
<ul>
<li><p>trained using <a class="reference external" href="http://aclweb.org/anthology/Q17-1010">the fastText algorithm</a></p></li>
<li><p>published by Facebook</p></li>
</ul>
</li>
</ul>
<p><strong>How to use pretrained embeddings</strong></p>
<p>Let’s try Google News pre-trained embeddings.</p>
<ul class="simple">
<li><p>You can download pre-trained embeddings from their original source.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Gensim</span></code> provides an api to conveniently load them. You need to install the <code class="docutils literal notranslate"><span class="pre">gensim</span></code> package in the course environment.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">conda-forge::gensim</span></code></p>
<p>If you get errors when you import <code class="docutils literal notranslate"><span class="pre">gensim</span></code>, try to install the following in the course environment.<br />
<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">--upgrade</span> <span class="pre">gensim</span> <span class="pre">scipy</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">gensim</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">gensim.downloader</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">api</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">info</span><span class="p">()[</span><span class="s2">&quot;models&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;fasttext-wiki-news-subwords-300&#39;, &#39;conceptnet-numberbatch-17-06-300&#39;, &#39;word2vec-ruscorpora-300&#39;, &#39;word2vec-google-news-300&#39;, &#39;glove-wiki-gigaword-50&#39;, &#39;glove-wiki-gigaword-100&#39;, &#39;glove-wiki-gigaword-200&#39;, &#39;glove-wiki-gigaword-300&#39;, &#39;glove-twitter-25&#39;, &#39;glove-twitter-50&#39;, &#39;glove-twitter-100&#39;, &#39;glove-twitter-200&#39;, &#39;__testing_word2vec-matrix-synopsis&#39;]
</pre></div>
</div>
</div>
</div>
<p>For the demonstration purpose, we’ll use <code class="docutils literal notranslate"><span class="pre">word2vec-google-news-300</span></code>. This model used to be available through <code class="docutils literal notranslate"><span class="pre">gensim.downloader</span></code>, but due to licensing and size (1.5+ GB), it’s no longer hosted on the default Gensim data server.</p>
<p>So I have manually download them (from <a class="reference external" href="https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300?resource=download&amp;amp;select=GoogleNews-vectors-negative300.bin.gz">here</a>) and then loaded them locally:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># It&#39;ll take a while to run this when you try it out for the first time.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">gensim.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">model_path</span> <span class="o">=</span> <span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;GoogleNews-vectors-negative300.bin.gz&quot;</span>
<span class="n">google_news_vectors</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Size of vocabulary: &quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">google_news_vectors</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Size of vocabulary:  3000000
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">google_news_vectors</span></code> above has 300 dimensional word vectors for 3,000,000 unique words/phrases from Google news.</p></li>
</ul>
<p><strong>What can we do with these word vectors?</strong></p>
<ul class="simple">
<li><p>Let’s examine word vector for the word UBC.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="p">[</span><span class="s2">&quot;UBC&quot;</span><span class="p">][:</span><span class="mi">20</span><span class="p">]</span>  <span class="c1"># Representation of the word UBC</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.3828125 , -0.18066406,  0.10644531,  0.4296875 ,  0.21582031,
       -0.10693359,  0.13476562, -0.08740234, -0.14648438, -0.09619141,
        0.02807617,  0.01409912, -0.12890625, -0.21972656, -0.41210938,
       -0.1875    , -0.11914062, -0.22851562,  0.19433594, -0.08642578],
      dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="p">[</span><span class="s2">&quot;UBC&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(300,)
</pre></div>
</div>
</div>
</div>
<p>It’s a short and a dense (we do not see any zeros) vector!</p>
<p><strong>Finding similar words</strong></p>
<ul class="simple">
<li><p>Given word <span class="math notranslate nohighlight">\(w\)</span>, search in the vector space for the word closest to <span class="math notranslate nohighlight">\(w\)</span> as measured by cosine similarity.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;UBC&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;UVic&#39;, 0.788647472858429),
 (&#39;SFU&#39;, 0.7588528394699097),
 (&#39;Simon_Fraser&#39;, 0.7356575131416321),
 (&#39;UFV&#39;, 0.6880435943603516),
 (&#39;VIU&#39;, 0.6778583526611328),
 (&#39;Kwantlen&#39;, 0.6771429181098938),
 (&#39;UBCO&#39;, 0.6734487414360046),
 (&#39;UPEI&#39;, 0.6731126308441162),
 (&#39;UBC_Okanagan&#39;, 0.6709135174751282),
 (&#39;Lakehead_University&#39;, 0.6622507572174072)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;information&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;info&#39;, 0.7363681793212891),
 (&#39;infomation&#39;, 0.680029571056366),
 (&#39;infor_mation&#39;, 0.673384964466095),
 (&#39;informaiton&#39;, 0.6639009118080139),
 (&#39;informa_tion&#39;, 0.660125732421875),
 (&#39;informationon&#39;, 0.633933424949646),
 (&#39;informationabout&#39;, 0.6320979595184326),
 (&#39;Information&#39;, 0.6186580657958984),
 (&#39;informaion&#39;, 0.6093292236328125),
 (&#39;details&#39;, 0.6063088774681091)]
</pre></div>
</div>
</div>
</div>
<p>If you want to extract all documents containing words similar to <strong>information</strong>, you could use this information.</p>
<p>Google News embeddings also support multi-word phrases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s2">&quot;british_columbia&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[(&#39;alberta&#39;, 0.6111124157905579),
 (&#39;canadian&#39;, 0.6086404323577881),
 (&#39;ontario&#39;, 0.6031432747840881),
 (&#39;erik&#39;, 0.5993571281433105),
 (&#39;dominican_republic&#39;, 0.5925410985946655),
 (&#39;costco&#39;, 0.5824530124664307),
 (&#39;rhode_island&#39;, 0.5804311633110046),
 (&#39;dreampharmaceuticals&#39;, 0.5755444765090942),
 (&#39;canada&#39;, 0.5630921721458435),
 (&#39;austin&#39;, 0.5623061656951904)]
</pre></div>
</div>
</div>
</div>
<p><strong>Finding similarity scores between words</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Canada&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.27610135
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Japan&quot;</span><span class="p">,</span> <span class="s2">&quot;hockey&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0019627847
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">word_pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;tall&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;official&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;mango&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pineapple&quot;</span><span class="p">,</span> <span class="s2">&quot;juice&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sun&quot;</span><span class="p">,</span> <span class="s2">&quot;robot&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;GPU&quot;</span><span class="p">,</span> <span class="s2">&quot;hummus&quot;</span><span class="p">),</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">word_pairs</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;The similarity between </span><span class="si">%s</span><span class="s2"> and </span><span class="si">%s</span><span class="s2"> is </span><span class="si">%0.3f</span><span class="s2">&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">google_news_vectors</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The similarity between height and tall is 0.473
The similarity between height and official is 0.002
The similarity between pineapple and mango is 0.668
The similarity between pineapple and juice is 0.418
The similarity between sun and robot is 0.029
The similarity between GPU and hummus is 0.094
</pre></div>
</div>
</div>
</div>
<p>We are getting reasonable word similarity scores!!</p>
<p><strong>Success of word2vec</strong></p>
<ul class="simple">
<li><p>This analogy example often comes up when people talk about word2vec, which was used by the authors of this method.</p></li>
<li><p><strong>MAN : KING :: WOMAN : ?</strong></p>
<ul>
<li><p>What is the word that is similar to <strong>WOMAN</strong> in the same sense as <strong>KING</strong> is similar to <strong>MAN</strong>?</p></li>
</ul>
</li>
<li><p>Perform a simple algebraic operations with the vector representation of words.
<span class="math notranslate nohighlight">\(\vec{X} = \vec{\text{KING}} − \vec{\text{MAN}} + \vec{\text{WOMAN}}\)</span></p></li>
<li><p>Search in the vector space for the word closest to <span class="math notranslate nohighlight">\(\vec{X}\)</span> measured by cosine distance.</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/word_analogies1.png"><img alt="../../_images/word_analogies1.png" src="../../_images/word_analogies1.png" style="width: 400px; height: 400px;" />
</a>
<p>(Credit: Mikolov et al. 2013)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">analogy</span><span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">google_news_vectors</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns analogy word using the given model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    --------------</span>
<span class="sd">    word1 : (str)</span>
<span class="sd">        word1 in the analogy relation</span>
<span class="sd">    word2 : (str)</span>
<span class="sd">        word2 in the analogy relation</span>
<span class="sd">    word3 : (str)</span>
<span class="sd">        word3 in the analogy relation</span>
<span class="sd">    model :</span>
<span class="sd">        word embedding model</span>

<span class="sd">    Returns</span>
<span class="sd">    ---------------</span>
<span class="sd">        pd.dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2"> : </span><span class="si">%s</span><span class="s2"> :: </span><span class="si">%s</span><span class="s2"> : ?&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">word1</span><span class="p">,</span> <span class="n">word2</span><span class="p">,</span> <span class="n">word3</span><span class="p">))</span>
    <span class="n">sim_words</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">word3</span><span class="p">,</span> <span class="n">word2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">word1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">sim_words</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Analogy word&quot;</span><span class="p">,</span> <span class="s2">&quot;Score&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;king&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : king :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>queen</td>
      <td>0.711819</td>
    </tr>
    <tr>
      <th>1</th>
      <td>monarch</td>
      <td>0.618967</td>
    </tr>
    <tr>
      <th>2</th>
      <td>princess</td>
      <td>0.590243</td>
    </tr>
    <tr>
      <th>3</th>
      <td>crown_prince</td>
      <td>0.549946</td>
    </tr>
    <tr>
      <th>4</th>
      <td>prince</td>
      <td>0.537732</td>
    </tr>
    <tr>
      <th>5</th>
      <td>kings</td>
      <td>0.523684</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Queen_Consort</td>
      <td>0.523595</td>
    </tr>
    <tr>
      <th>7</th>
      <td>queens</td>
      <td>0.518113</td>
    </tr>
    <tr>
      <th>8</th>
      <td>sultan</td>
      <td>0.509859</td>
    </tr>
    <tr>
      <th>9</th>
      <td>monarchy</td>
      <td>0.508741</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Montreal&quot;</span><span class="p">,</span> <span class="s2">&quot;Canadiens&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Montreal : Canadiens :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Canucks</td>
      <td>0.821327</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Vancouver_Canucks</td>
      <td>0.750401</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Calgary_Flames</td>
      <td>0.705470</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Leafs</td>
      <td>0.695783</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Maple_Leafs</td>
      <td>0.691617</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Thrashers</td>
      <td>0.687504</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Avs</td>
      <td>0.681716</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Sabres</td>
      <td>0.665307</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Blackhawks</td>
      <td>0.664625</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Habs</td>
      <td>0.661023</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Toronto&quot;</span><span class="p">,</span> <span class="s2">&quot;UofT&quot;</span><span class="p">,</span> <span class="s2">&quot;Vancouver&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Toronto : UofT :: Vancouver : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>SFU</td>
      <td>0.579245</td>
    </tr>
    <tr>
      <th>1</th>
      <td>UVic</td>
      <td>0.576921</td>
    </tr>
    <tr>
      <th>2</th>
      <td>UBC</td>
      <td>0.571431</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Simon_Fraser</td>
      <td>0.543464</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Langara_College</td>
      <td>0.541347</td>
    </tr>
    <tr>
      <th>5</th>
      <td>UVIC</td>
      <td>0.520495</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Grant_MacEwan</td>
      <td>0.517273</td>
    </tr>
    <tr>
      <th>7</th>
      <td>UFV</td>
      <td>0.514150</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Ubyssey</td>
      <td>0.510421</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Kwantlen</td>
      <td>0.503807</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;Gauss&quot;</span><span class="p">,</span> <span class="s2">&quot;mathematician&quot;</span><span class="p">,</span> <span class="s2">&quot;Bob_Dylan&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gauss : mathematician :: Bob_Dylan : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>singer_songwriter_Bob_Dylan</td>
      <td>0.520782</td>
    </tr>
    <tr>
      <th>1</th>
      <td>poet</td>
      <td>0.501191</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Pete_Seeger</td>
      <td>0.497143</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Joan_Baez</td>
      <td>0.492307</td>
    </tr>
    <tr>
      <th>4</th>
      <td>sitarist_Ravi_Shankar</td>
      <td>0.491968</td>
    </tr>
    <tr>
      <th>5</th>
      <td>bluesman</td>
      <td>0.490930</td>
    </tr>
    <tr>
      <th>6</th>
      <td>jazz_musician</td>
      <td>0.489593</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Joni_Mitchell</td>
      <td>0.487740</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Billie_Holiday</td>
      <td>0.486664</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Johnny_Cash</td>
      <td>0.485722</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So you can imagine these models being useful in many meaning-related tasks.</p>
<p><strong>Implicit biases and stereotypes in word embeddings</strong></p>
<p>Embeddings reflect biases in the data they are trained on.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;computer_programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : computer_programmer :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>homemaker</td>
      <td>0.562712</td>
    </tr>
    <tr>
      <th>1</th>
      <td>housewife</td>
      <td>0.510505</td>
    </tr>
    <tr>
      <th>2</th>
      <td>graphic_designer</td>
      <td>0.505180</td>
    </tr>
    <tr>
      <th>3</th>
      <td>schoolteacher</td>
      <td>0.497949</td>
    </tr>
    <tr>
      <th>4</th>
      <td>businesswoman</td>
      <td>0.493489</td>
    </tr>
    <tr>
      <th>5</th>
      <td>paralegal</td>
      <td>0.492551</td>
    </tr>
    <tr>
      <th>6</th>
      <td>registered_nurse</td>
      <td>0.490797</td>
    </tr>
    <tr>
      <th>7</th>
      <td>saleswoman</td>
      <td>0.488163</td>
    </tr>
    <tr>
      <th>8</th>
      <td>electrical_engineer</td>
      <td>0.479773</td>
    </tr>
    <tr>
      <th>9</th>
      <td>mechanical_engineer</td>
      <td>0.475540</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><img alt="" src="../../_images/eva-srsly.png" /></p>
<ul class="simple">
<li><p>Embeddings reflect gender stereotypes present in broader society.</p></li>
<li><p>They may also amplify these stereotypes because of their widespread usage.</p></li>
<li><p>See the paper <a class="reference external" href="http://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf">Man is to Computer Programmer as Woman is to …</a>.</p></li>
</ul>
<p>Most of the modern embeddings are de-biased for some obvious biases. For example, we won’t see this with <code class="docutils literal notranslate"><span class="pre">glove_wiki_vectors</span></code>. We changed “computer_programmer” to “programmer” because
“computer_programmer” is not in the vocabulary of <code class="docutils literal notranslate"><span class="pre">glove_wiki_vectors</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">glove_wiki_vectors</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;glove-wiki-gigaword-300&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[==================================================] 100.0% 376.1/376.1MB downloaded
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">glove_wiki_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>400000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">analogy</span><span class="p">(</span><span class="s2">&quot;man&quot;</span><span class="p">,</span> <span class="s2">&quot;programmer&quot;</span><span class="p">,</span> <span class="s2">&quot;woman&quot;</span><span class="p">,</span> <span class="n">model</span> <span class="o">=</span> <span class="n">glove_wiki_vectors</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>man : programmer :: woman : ?
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Analogy word</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>programmers</td>
      <td>0.497601</td>
    </tr>
    <tr>
      <th>1</th>
      <td>freelance</td>
      <td>0.417259</td>
    </tr>
    <tr>
      <th>2</th>
      <td>educator</td>
      <td>0.403169</td>
    </tr>
    <tr>
      <th>3</th>
      <td>businesswoman</td>
      <td>0.392910</td>
    </tr>
    <tr>
      <th>4</th>
      <td>designer</td>
      <td>0.392894</td>
    </tr>
    <tr>
      <th>5</th>
      <td>translator</td>
      <td>0.385843</td>
    </tr>
    <tr>
      <th>6</th>
      <td>technician</td>
      <td>0.375108</td>
    </tr>
    <tr>
      <th>7</th>
      <td>computer</td>
      <td>0.374914</td>
    </tr>
    <tr>
      <th>8</th>
      <td>animator</td>
      <td>0.367700</td>
    </tr>
    <tr>
      <th>9</th>
      <td>homemaker</td>
      <td>0.367547</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><br><br></p>
</section>
<section id="beyond-words-sentence-embeddings">
<h3>Beyond words: sentence embeddings<a class="headerlink" href="#beyond-words-sentence-embeddings" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Word embeddings capture individual word meanings.</p></li>
<li><p>But what about sentences or paragraphs?</p></li>
<li><p>Modern deep learning models represent whole texts using sentence embeddings, where the meaning of a sentence is captured in a single vector. This is what you used in HW6.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span><span class="p">,</span> <span class="n">util</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s2">&quot;all-MiniLM-L6-v2&quot;</span><span class="p">)</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="s2">&quot;The cat sat on the mat.&quot;</span><span class="p">,</span>
<span class="s2">&quot;A feline rested on the rug.&quot;</span><span class="p">,</span>
<span class="s2">&quot;I love teaching you machine learning.&quot;</span><span class="p">,</span>
<span class="s2">&quot;Machine learning is fascinating.&quot;</span>
<span class="p">]</span>

<span class="c1"># Compute embeddings</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>

<span class="c1"># Compute pairwise cosine similarities</span>
<span class="n">similarities</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">cos_sim</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="c1"># Create a table</span>
<span class="n">df_sim</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">sentences</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">sentences</span><span class="p">)</span>
<span class="n">df_sim</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "862a581405a348acad34940a95b40771", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Giulia\anaconda3\envs\cpsc330\Lib\site-packages\huggingface_hub\file_download.py:142: UserWarning:

`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Giulia\.cache\huggingface\hub\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "bd2f3c075c0946f8aaefee31c4ea54ec", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "c7e6f3d061394f51830a4046319f56db", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "77c96d44d4b4447c9f90ba1950c8999a", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "972bbba044674e83a3b6a8f93f9d599a", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "4424beddd34549b0a706180baa54a461", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "85a6633c92204e4aaf1a299d8362cbc1", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "6bd02af3afe049d9931b11dd38d78af4", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "651821a020dc4aa0bd11453dafa34800", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a846f6c6948540fdba6fafbc466cc781", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "6203dc6978d14bb4b3b366f3d0579201", "version_major": 2, "version_minor": 0}</script><div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>The cat sat on the mat.</th>
      <th>A feline rested on the rug.</th>
      <th>I love teaching you machine learning.</th>
      <th>Machine learning is fascinating.</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>The cat sat on the mat.</th>
      <td>1.000</td>
      <td>0.556</td>
      <td>-0.009</td>
      <td>-0.023</td>
    </tr>
    <tr>
      <th>A feline rested on the rug.</th>
      <td>0.556</td>
      <td>1.000</td>
      <td>-0.009</td>
      <td>-0.059</td>
    </tr>
    <tr>
      <th>I love teaching you machine learning.</th>
      <td>-0.009</td>
      <td>-0.009</td>
      <td>1.000</td>
      <td>0.732</td>
    </tr>
    <tr>
      <th>Machine learning is fascinating.</th>
      <td>-0.023</td>
      <td>-0.059</td>
      <td>0.732</td>
      <td>1.000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><br><br></p>
</section>
</section>
<section id="introduction-to-large-language-models">
<h2>Introduction to large language models<a class="headerlink" href="#introduction-to-large-language-models" title="Link to this heading">#</a></h2>
<section id="language-models-activity">
<h3>Language models activity<a class="headerlink" href="#language-models-activity" title="Link to this heading">#</a></h3>
<p>Let’s start with a game!</p>
<p>Some of you will receive a sticky note with a word on it. Here’s what to do:</p>
<ol class="arabic simple">
<li><p><strong>Look at your word.</strong> Don’t show it to anyone!</p></li>
<li><p>Think quickly: what word might logically follow this one?<br />
✍️ Write your predicted next word on a new sticky note.</p></li>
<li><p>You have 20 seconds. Trust your instincts.</p></li>
<li><p><strong>Pass your predicted word</strong> to the person next to you (not the one you received).</p></li>
<li><p>Continue until the last person in your row has written their word.</p></li>
</ol>
<p><br><br><br><br></p>
<p>You’ve just created a <strong>simple Markov model of language</strong> — each person predicted the next word based only on limited context.</p>
<blockquote>
<div><p>“I saw the word <em>data</em> <span class="math notranslate nohighlight">\(\rightarrow\)</span> I wrote <em>science</em>.”<br />
“I saw the word <em>machine</em> <span class="math notranslate nohighlight">\(\rightarrow\)</span> I wrote <em>learning</em>.”</p>
</div></blockquote>
<p>This is how early language models worked: predict the next word using local context and co-occurrence probabilities.</p>
<p><br><br></p>
</section>
<section id="language-model">
<h3>Language model<a class="headerlink" href="#language-model" title="Link to this heading">#</a></h3>
<p>A language model computes the probability distribution over sequences (of words or characters). Intuitively, this probability tells us how “good” or plausible a sequence of words is.</p>
<p><img alt="" src="../../_images/voice-assistant-ex.png" /></p>
<p>Check out this <a class="reference external" href="https://www.youtube.com/watch?v=VzqKtAYeJt4">recent BMO ad</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif&quot;</span>

<span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">IFrame</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
        <iframe
            width="500"
            height="500"
            src="https://2.bp.blogspot.com/-KlBuhzV_oFw/WvxP_OAkJ1I/AAAAAAAACu0/T0F6lFZl-2QpS0O7VBMhf8wkUPvnRaPIACLcBGAs/s1600/image2.gif"
            frameborder="0"
            allowfullscreen
            
        ></iframe>
        </div></div>
</div>
</section>
<section id="a-simple-model-of-language">
<h3>A simple model of language<a class="headerlink" href="#a-simple-model-of-language" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Calculate the co-occurrence frequencies and probabilities based on these frequencies</p></li>
<li><p>Predict the next word based on these probabilities</p></li>
<li><p>This is a Markov model of language.</p></li>
</ul>
<p><img alt="" src="../../_images/Markov-bigram-probs.png" /></p>
</section>
<section id="from-markov-models-to-meaning">
<h3>From Markov models to meaning<a class="headerlink" href="#from-markov-models-to-meaning" title="Link to this heading">#</a></h3>
<p>Markov models can predict short sequences, but they quickly fall apart with longer context.</p>
<p>For example:</p>
<blockquote>
<div><p>“I am studying law at the University of British Columbia because I want to work as a ___”</p>
</div></blockquote>
<p>To predict the last word (<em>lawyer</em>), we must remember information from the <em>beginning</em> of the sentence — something a simple Markov model can’t do.</p>
<blockquote>
<div><p>We need models that can <strong>remember long-range dependencies</strong> and <strong>weigh context</strong> differently.</p>
</div></blockquote>
</section>
<section id="from-word-prediction-to-transformers">
<h3>From word prediction to transformers<a class="headerlink" href="#from-word-prediction-to-transformers" title="Link to this heading">#</a></h3>
<p>Earlier deep learning models like Recurrent Neural Networks (RNNs) and Long-Short Term Memory Models (LSTMs) tried to solve this by “remembering” previous words. But they process words one at a time, making them slow and still forgetful.</p>
<p><strong>Transformer models</strong> changed everything. They read all words <strong>in parallel</strong> and use <strong>attention</strong> to decide which words to focus on. Transformer architectures are at the heart of today’s most powerful generative AI models (GPT-4, GPT-5, Gemini, LLaMA, Claude, and many others).</p>
<p>Check out <a class="reference external" href="https://www.youtube.com/watch?v=NJ_kTPwcaJU">this video on self-attention</a> if you want to know more.</p>
<p><img alt="" src="../../_images/genai.png" /></p>
</section>
<section id="what-are-large-language-models-llms">
<h3>What are Large Language Models (LLMs)?<a class="headerlink" href="#what-are-large-language-models-llms" title="Link to this heading">#</a></h3>
<p>A <strong>Large Language Model (LLM)</strong> is a neural network trained to predict the next token in a sequence.</p>
<p>By doing this billions of times across massive text corpora, the model learns:</p>
<ul class="simple">
<li><p>grammar and syntax</p></li>
<li><p>world knowledge</p></li>
<li><p>relationships between concepts</p></li>
<li><p>even reasoning patterns</p></li>
</ul>
<p><img alt="" src="../../_images/GPT-4-tech-report-abstract.png" /><br />
Source: <a class="reference external" href="https://arxiv.org/pdf/2303.08774.pdf">GPT-4 Technical Report (OpenAI, 2023)</a></p>
</section>
<section id="common-architectures">
<h3>Common architectures<a class="headerlink" href="#common-architectures" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Decoder-only</p></th>
<th class="head"><p>Encoder-only</p></th>
<th class="head"><p>Encoder-decoder</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Examples</strong></p></td>
<td><p>GPT-3, LLaMA, Gemini</p></td>
<td><p>BERT, RoBERTa</p></td>
<td><p>T5, BART</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Uses</strong></p></td>
<td><p>Text generation, chatbots</p></td>
<td><p>Text classification, embeddings</p></td>
<td><p>Translation, summarization</p></td>
</tr>
<tr class="row-even"><td><p><strong>Context Handling</strong></p></td>
<td><p>Considers earlier tokens</p></td>
<td><p>Bidirectional (full context)</p></td>
<td><p>Encodes input, generates output</p></td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<div><p>Most generative models you use (ChatGPT, Claude, Gemini) are <strong>decoder-only transformers</strong>.</p>
</div></blockquote>
</section>
<section id="nlp-pipelines-before-and-after-llms">
<h3>NLP pipelines before and after LLMs<a class="headerlink" href="#nlp-pipelines-before-and-after-llms" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Traditional NLP Pipeline</p></th>
<th class="head"><p>LLM-Powered Pipeline</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Text preprocessing, tokenization</p></td>
<td><p>Minimal preprocessing</p></td>
</tr>
<tr class="row-odd"><td><p>Feature extraction (BoW, TF-IDF, embeddings)</p></td>
<td><p>Implicit contextual embeddings</p></td>
</tr>
<tr class="row-even"><td><p>One model per task</p></td>
<td><p>One model, many tasks</p></td>
</tr>
<tr class="row-odd"><td><p>Needs labeled data</p></td>
<td><p>Zero-shot and few-shot learning</p></td>
</tr>
</tbody>
</table>
</div>
<p>LLMs have shifted NLP from <strong>feature engineering</strong> to <strong>prompt engineering</strong>.</p>
<p>There are many Python libraries that make it easy to use pretrained LLMs:</p>
<ul class="simple">
<li><p>🤗 <a class="reference external" href="https://huggingface.co/docs/transformers/index"><strong>Transformers</strong></a> — unified interface for hundreds of models</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/openai/"><strong>OpenAI API</strong></a> — GPT-3.5 / GPT-4 models</p></li>
<li><p><a class="reference external" href="https://python.langchain.com/v0.2/docs/introduction/"><strong>LangChain</strong></a> — building complex LLM workflows</p></li>
<li><p><a class="reference external" href="https://pypi.org/project/farm-haystack/"><strong>Haystack</strong></a> — retrieval-augmented generation (RAG)</p></li>
<li><p><a class="reference external" href="https://spacy.io/universe/project/spacy-transformers"><strong>spaCy Transformers</strong></a> — NLP with transformer backends
engineering**.
<br><br></p></li>
</ul>
</section>
<section id="example-sentiment-analysis-using-a-pretrained-model">
<h3>Example: Sentiment analysis using a pretrained model<a class="headerlink" href="#example-sentiment-analysis-using-a-pretrained-model" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">AutoModelForTokenClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="c1"># Sentiment analysis pipeline</span>
<span class="n">analyzer</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;distilbert-base-uncased-finetuned-sst-2-english&#39;</span><span class="p">)</span>
<span class="n">analyzer</span><span class="p">([</span><span class="s2">&quot;I asked my model to predict my future, and it said &#39;404: Life not found.&#39;&quot;</span><span class="p">,</span>
<span class="w">          </span><span class="sd">&#39;&#39;&#39;Machine learning is just like cooking—sometimes you follow the recipe, </span>
<span class="sd">            and other times you just hope for the best!.&#39;&#39;&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d8eaad02d1084407aedaa579bc7f4ddd", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Giulia\anaconda3\envs\cpsc330\Lib\site-packages\huggingface_hub\file_download.py:142: UserWarning:

`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Giulia\.cache\huggingface\hub\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6e7e805e4abe443b99862398fd2504af", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ee0b4cb7b83243f49a5cc79ff476b6f5", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "abb433c5ccec42fe87447cf227596754", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device set to use cpu
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.995707631111145},
 {&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9994770884513855}]
</pre></div>
</div>
</div>
</div>
<p>Now let’s try emotion classification</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;dair-ai/emotion&quot;</span><span class="p">)</span>
<span class="n">exs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">][</span><span class="mi">3</span><span class="p">:</span><span class="mi">15</span><span class="p">]</span>
<span class="n">exs</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "8fda6bf6e47d437ab84abea35ae6b27a", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Giulia\anaconda3\envs\cpsc330\Lib\site-packages\huggingface_hub\file_download.py:142: UserWarning:

`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Giulia\.cache\huggingface\hub\datasets--dair-ai--emotion. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "d28b42a3749848b9bc3b30c31f0ddb6d", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "623912b81ab7452d948ee557cdd75485", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ec1a08a238a044ed9ae7e31562849c2b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "275056f885e5429c9e8ee2dc2eaeb4ed", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "1527a29fde054485a7c0c90dd4dd1ec9", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ee3797bc0cf54839beb212dad5c64c3b", "version_major": 2, "version_minor": 0}</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived&#39;,
 &#39;i was feeling a little vain when i did this one&#39;,
 &#39;i cant walk into a shop anywhere where i do not feel uncomfortable&#39;,
 &#39;i felt anger when at the end of a telephone call&#39;,
 &#39;i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting accepted into the masters program at the university of virginia&#39;,
 &#39;i like to have the same breathless feeling as a reader eager to see what will happen next&#39;,
 &#39;i jest i feel grumpy tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer&#39;,
 &#39;i don t feel particularly agitated&#39;,
 &#39;i feel beautifully emotional knowing that these women of whom i knew just a handful were holding me and my baba on our journey&#39;,
 &#39;i pay attention it deepens into a feeling of being invaded and helpless&#39;,
 &#39;i just feel extremely comfortable with the group of people that i dont even need to hide myself&#39;,
 &#39;i find myself in the odd position of feeling supportive of&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pipeline</span> 
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1">#Load the pretrained model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;facebook/bart-large-mnli&quot;</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s1">&#39;zero-shot-classification&#39;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">exs</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">candidate_labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;sadness&quot;</span><span class="p">,</span> <span class="s2">&quot;joy&quot;</span><span class="p">,</span> <span class="s2">&quot;love&quot;</span><span class="p">,</span><span class="s2">&quot;anger&quot;</span><span class="p">,</span> <span class="s2">&quot;fear&quot;</span><span class="p">,</span> <span class="s2">&quot;surprise&quot;</span><span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">exs</span><span class="p">,</span> <span class="n">candidate_labels</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6073db1a375c4f27951eef225b6464e4", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Giulia\anaconda3\envs\cpsc330\Lib\site-packages\huggingface_hub\file_download.py:142: UserWarning:

`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\Users\Giulia\.cache\huggingface\hub\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.
To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "c8c134f92e554c699467539fcb56e850", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "941b849c2d894620b4761e9b55c609b0", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "56e43925ef4f42fe9c1e9c83dccd048b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "718a5f5f98524af2a36c68e697226733", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "3f23cf793a334d17b1e26773efb9d7cf", "version_major": 2, "version_minor": 0}</script><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Device set to use cpu
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sequence</th>
      <th>labels</th>
      <th>scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>im feeling rather rotten so im not very ambiti...</td>
      <td>[sadness, anger, surprise, fear, joy, love]</td>
      <td>[0.7367969155311584, 0.10041749477386475, 0.09...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>im updating my blog because i feel shitty</td>
      <td>[sadness, surprise, anger, fear, joy, love]</td>
      <td>[0.7429753541946411, 0.1377594769001007, 0.058...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>i never make her separate from me because i do...</td>
      <td>[love, sadness, surprise, fear, anger, joy]</td>
      <td>[0.3153638541698456, 0.22490352392196655, 0.19...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>i left with my bouquet of red and yellow tulip...</td>
      <td>[surprise, joy, love, sadness, fear, anger]</td>
      <td>[0.42182019352912903, 0.3336699604988098, 0.21...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>i was feeling a little vain when i did this one</td>
      <td>[surprise, anger, fear, love, joy, sadness]</td>
      <td>[0.5639421939849854, 0.17000249028205872, 0.08...</td>
    </tr>
    <tr>
      <th>5</th>
      <td>i cant walk into a shop anywhere where i do no...</td>
      <td>[surprise, fear, sadness, anger, joy, love]</td>
      <td>[0.37033477425575256, 0.36559349298477173, 0.1...</td>
    </tr>
    <tr>
      <th>6</th>
      <td>i felt anger when at the end of a telephone call</td>
      <td>[anger, surprise, fear, sadness, joy, love]</td>
      <td>[0.9760519862174988, 0.012534401379525661, 0.0...</td>
    </tr>
    <tr>
      <th>7</th>
      <td>i explain why i clung to a relationship with a...</td>
      <td>[surprise, joy, love, sadness, fear, anger]</td>
      <td>[0.4382021725177765, 0.23223206400871277, 0.12...</td>
    </tr>
    <tr>
      <th>8</th>
      <td>i like to have the same breathless feeling as ...</td>
      <td>[surprise, joy, love, fear, anger, sadness]</td>
      <td>[0.7675780653953552, 0.13846909999847412, 0.03...</td>
    </tr>
    <tr>
      <th>9</th>
      <td>i jest i feel grumpy tired and pre menstrual w...</td>
      <td>[surprise, sadness, anger, fear, joy, love]</td>
      <td>[0.7340192198753357, 0.11860274523496628, 0.07...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Harms of large language models</strong></p>
<p>While these models are super powerful and useful, be mindful of the harms caused by these models. Some of the harms as summarized [here]:</p>
<ul class="simple">
<li><p>performance disparties</p></li>
<li><p>social biases and stereotypes</p></li>
<li><p>toxicity</p></li>
<li><p>misinformation</p></li>
<li><p>security and privacy risks</p></li>
<li><p>copyright and legal protections</p></li>
<li><p>environmental impact</p></li>
<li><p>centralization of power</p></li>
</ul>
<p>For more, see Stanford CS324 Lecture on <a class="reference external" href="https://stanford-cs324.github.io/winter2022/lectures/harms-1/">Harms of LLMs</a>.</p>
<p><strong>Takeaway message</strong></p>
<ul class="simple">
<li><p>Language modeling began as simple next-word prediction.</p></li>
<li><p>Transformers introduced self-attention for contextual understanding.</p></li>
<li><p>LLMs scaled these ideas to billions of parameters, enabling reasoning and generation.</p></li>
<li><p>With great power comes great responsibility — awareness and ethical use are key.</p></li>
</ul>
<p><br><br><br><br></p>
</section>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>NLP is a big and very active field.</p></li>
<li><p>We broadly explored three topics:</p>
<ul>
<li><p>Topic modeling</p></li>
<li><p>Word and text representations embeddings using pretrained models</p></li>
<li><p>Introduction to large language models</p></li>
</ul>
</li>
</ul>
<p>Here are some resources if you want to get into NLP.</p>
<ul class="simple">
<li><p>Check out this <a class="reference external" href="https://www.cs.ubc.ca/~vshwartz/courses/CPSC436N-22/index.html">CPSC course on NLP</a>.</p></li>
<li><p>The first resource I would recommend is the following book by Jurafsky and Martin. It’s very approachable and fun. And the current edition is available online.</p>
<ul>
<li><p><a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing by Dan Jurafsky and James H. Martin</a></p></li>
</ul>
</li>
<li><p>There is a course taught at Stanford called <a class="reference external" href="http://web.stanford.edu/class/cs124/">“From languages to Information”</a> by one of the co-authors of the above book, and it might be a good introduction to NLP for you. Most of the <a class="reference external" href="https://www.youtube.com/playlist?list=PLoROMvodv4rOFZnDyrlW3-nI7tMLtmiJZ&amp;amp;disable_polymer=true">course material</a> and <a class="reference internal" href="#"><span class="xref myst">videos</span></a> are available for free.</p></li>
<li><p>If you are into deep learning, you may refer to <a class="reference external" href="https://cs224d.stanford.edu/">this course</a>. Again, all lecture videos are available on youtube.</p></li>
<li><p>If you want to look at current advancements in the field, you’ll find all NLP related publications <a class="reference external" href="https://www.aclweb.org/anthology/">here</a>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "cpsc330"
        },
        kernelOptions: {
            name: "cpsc330",
            path: "./lectures/101-103-Giulia-lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'cpsc330'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-natural-language-processing-nlp">What is Natural Language Processing (NLP)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling">Topic modeling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-motivation">Topic modeling motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lda-topics-in-social-media">LDA topics in social media</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-toy-example">Topic modeling toy example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#topic-modeling-pipeline">Topic modeling pipeline</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#text-representations-and-word-embeddings">Text representations and word embeddings</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">Motivation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distributional-hypothesis">Distributional hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embeddings-the-idea">Word embeddings: The idea</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#measuring-similarity-between-vectors">Measuring similarity between vectors</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion-question">Discussion question</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-word-embeddings">Using pre-trained word embeddings</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-words-sentence-embeddings">Beyond words: sentence embeddings</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-large-language-models">Introduction to large language models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-models-activity">Language models activity</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#language-model">Language model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-simple-model-of-language">A simple model of language</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-markov-models-to-meaning">From Markov models to meaning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#from-word-prediction-to-transformers">From word prediction to transformers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-large-language-models-llms">What are Large Language Models (LLMs)?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-architectures">Common architectures</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nlp-pipelines-before-and-after-llms">NLP pipelines before and after LLMs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sentiment-analysis-using-a-pretrained-model">Example: Sentiment analysis using a pretrained model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>