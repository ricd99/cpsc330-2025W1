
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lecture 18: Multi-class classification and introduction to computer vision &#8212; CPSC 330 Applied Machine Learning 2025W1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/extra.css?v=6df0ab2b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/101-103-Giulia-lectures/18_intro_to_computer-vision';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/UBC-CS-logo.png" class="logo__image only-light" alt="CPSC 330 Applied Machine Learning 2025W1 - Home"/>
    <script>document.write(`<img src="../../_static/UBC-CS-logo.png" class="logo__image only-dark" alt="CPSC 330 Applied Machine Learning 2025W1 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    UBC CPSC 330: Applied Machine Learning (2025W1)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/README.html">CPSC 330 Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../learning-objectives.html">Course Learning Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notes/01_intro.html">Lecture 1: Course Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/02_terminology-decision-trees.html">Lecture 2: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/03_ml-fundamentals.html">Lecture 3: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/04_kNNs-SVM-RBF.html">Lecture 4: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/05_preprocessing-pipelines.html">Lecture 5: Preprocessing and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/06_column-transformer-text-feats.html">Lecture 6: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and Text Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/07_linear-models.html">Lecture 7: Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/08_hyperparameter-optimization.html">Lecture 8: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/09_classification-metrics.html">Lecture 9: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/10_regression-metrics.html">Lecture 10: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/11_ensembles.html">Lecture 11: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/12_feat-importances.html">Lecture 12: Feature importances and model transparency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/13_feature-engineering-selection.html">Lecture 13: Feature engineering and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/14_K-Means.html">Lecture 14: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/15_DBSCAN-hierarchical.html">Lecture 15: More Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/16_recommender-systems.html">Lecture 16: Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/17_natural-language-processing.html">Lecture 17: Introduction to natural language processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/18_intro_to_computer-vision.html">Lecture 18: Multi-class classification and introduction to computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/19_time-series.html">Lecture 19: Time series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/20_survival-analysis.html">Lecture 20: Survival analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/21_communication.html">Lecture 21: Communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/23_deployment-conclusion.html">Lecture 23: Deployment and conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/final-exam-review-guiding-question.html">Final exam preparation: guiding questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixA.html">Appendix A: Handling class imbalance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixB.html">Appendix B: Feature Selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixC.html">Appendix C: Basic text preprocessing [video]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/AppendixD.html">Appendix D: Multi-class, meta-strategies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Section slides</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../102-Varada-lectures/README.html">Section 102</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-01.html">Lecture 1</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-02.html">Lecture 2</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-03.html">Lecture 3</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-04.html">Lecture 4</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-05.html">Lecture 5</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-06.html">Lecture 6</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-07.html">Lecture 7</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-08.html">Lecture 8</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-09.html">Lecture 9</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-10.html">Lecture 10</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-11.html">Lecture 11</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-12.html">Lecture 12</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-13.html">Lecture 13</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-14.html">Lecture 14</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-15.html">Lecture 15</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-16.html">Lecture 16</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-17.html">Lecture 17</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-18.html">Lecture 18</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-19.html">Lecture 19</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-20.html">Lecture 20</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-21.html">Lecture 21</a></li>
<li class="toctree-l2"><a class="reference external" href="https://kvarada.github.io/cpsc330-slides/lecture-23.html">Lecture 23</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-CS/cpsc330-2025W1" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/101-103-Giulia-lectures/18_intro_to_computer-vision.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 18: Multi-class classification and introduction to computer vision</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports-and-lo">Imports and LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise">iClicker Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-classification">Multi-class classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">iClicker Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-vs-softmax">Sigmoid vs. Softmax</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-neural-networks">Introduction to neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-example-and-terminology">Neural networks example and terminology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-how-does-training-work-in-neural-networks-high-level">(Optional) How does training work in neural networks? (High-Level)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-neural-networks">Why neural networks?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-neural-networks">Why not neural networks?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-software">Deep learning software</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-computer-vision">Introduction to computer vision</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns-high-level">Convolutional Neural Networks (CNNs) (high level)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-models-out-of-the-box">Using pre-trained models out-of-the-box</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-models-as-feature-extractor">Using pre-trained models as feature extractor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection">Object detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p><img alt="" src="../../_images/330-banner.png" /></p>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-18-multi-class-classification-and-introduction-to-computer-vision">
<h1>Lecture 18: Multi-class classification and introduction to computer vision<a class="headerlink" href="#lecture-18-multi-class-classification-and-introduction-to-computer-vision" title="Link to this heading">#</a></h1>
<section id="imports-and-lo">
<h2>Imports and LO<a class="headerlink" href="#imports-and-lo" title="Link to this heading">#</a></h2>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">glob</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">),</span> <span class="s2">&quot;code&quot;</span><span class="p">))</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">plotting_functions</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deep_learning_code</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.dummy</span><span class="w"> </span><span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="n">DATA_DIR</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;..&quot;</span><span class="p">),</span> <span class="s2">&quot;data/&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="learning-objectives">
<h3>Learning objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Describe what multiclass classification is and how it differs from binary classification</p></li>
<li><p>Explain how logisitic regression prediction is extended to the multiclass setting (multinominal/softmax) and roughly count its parameters</p></li>
<li><p>Explain the basics of neural networks and how they build upon linear models.</p></li>
<li><p>Explain at a high level what makes a model “deep”</p></li>
<li><p>Describe the role of neural networks in machine learning, including their strengths and limitations.</p></li>
<li><p>Explain the intuition behind convolutional neural networks: what filters do, how they slide over an image and why this helps with images</p></li>
<li><p>Explain why the methods we’ve learned previously would not be effective on image data.</p></li>
<li><p>Define transfer learning and explain why pre-trained models are useful in vision and NLP.</p></li>
<li><p>Apply pre-trained neural networks to classification and regression problems.</p></li>
<li><p>Utilize pre-trained networks as feature extractors and train simple classifier (e.g., logistic regression) on top</p></li>
</ul>
</section>
</section>
<section id="questions-for-you">
<h2>❓❓ Questions for you<a class="headerlink" href="#questions-for-you" title="Link to this heading">#</a></h2>
<section id="iclicker-exercise">
<h3>iClicker Exercise<a class="headerlink" href="#iclicker-exercise" title="Link to this heading">#</a></h3>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) It’s possible to use word2vec embedding representations for text classification instead of bag-of-words representation.</p></li>
<li><p>(B) The topic model approach we used in the last lecture, Latent Dirichlet Allocation (LDA), is an unsupervised approach.</p></li>
<li><p>(C) In an LDA topic model, the same word can be associated with two different topics with high probability.</p></li>
<li><p>(D) In an LDA topic model, a document is a mixture of multiple topics.</p></li>
<li><p>(E) If I train a topic model on a large collection of news articles with K = 10, I would get 10 topic labels (e.g., sports, culture, politics, finance) as output.</p></li>
</ul>
<p><br><br></p>
</section>
</section>
<section id="multi-class-classification">
<h2>Multi-class classification<a class="headerlink" href="#multi-class-classification" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>So far, we’ve focused on binary classification (two classes).</p></li>
<li><p>Can we use these classifiers when there are more than two classes?</p>
<ul>
<li><p><a class="reference external" href="http://www.image-net.org/challenges/LSVRC/">“ImageNet” computer vision competition</a>, for example, has 1000 classes</p></li>
</ul>
</li>
<li><p>The goal of multi-class classification is to assign each example to exactly one of <span class="math notranslate nohighlight">\(K\)</span> possible classes.</p></li>
</ul>
<section id="id1">
<h3>iClicker Exercise<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Which classifiers can already handle more than two classes?</p>
<ul class="simple">
<li><p>(A) kNN</p></li>
<li><p>(B) Decision trees (including ensamble tree based models)</p></li>
<li><p>(C) SVM</p></li>
<li><p>(D) Logistic regression</p></li>
</ul>
<ul class="simple">
<li><p>Some models naturally extend to multiclass classification.</p></li>
<li><p>If they don’t a common technique is to reduce multiclass classication into several instances of binary classification problems.</p></li>
<li><p>Two kind of “hacky” ways to reduce multi-class classification into binary classification:</p>
<ul>
<li><p>the one-vs.-rest approach</p></li>
<li><p>the one-vs.-one approach</p></li>
</ul>
</li>
<li><p>Check out <a class="reference internal" href="#AppendixD.html"><span class="xref myst">AppendixD</span></a> for more details.</p></li>
</ul>
<p>Let’s look at a multiclass classification with logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mglearn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_blobs</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">centers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span>
<span class="p">)</span>
<span class="n">mglearn</span><span class="o">.</span><span class="n">discrete_scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature 0&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Feature 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s2">&quot;Class 0&quot;</span><span class="p">,</span> <span class="s2">&quot;Class 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Class 2&quot;</span><span class="p">,</span> <span class="s2">&quot;Class 3&quot;</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/76a13e151fb44859b39d45766cc4c6d379938b7ff7ddbeb29e47ef65b546301c.png" src="../../_images/76a13e151fb44859b39d45766cc4c6d379938b7ff7ddbeb29e47ef65b546301c.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: #000;
  --sklearn-color-text-muted: #666;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: flex;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
  align-items: start;
  justify-content: space-between;
  gap: 0.5em;
}

#sk-container-id-1 label.sk-toggleable__label .caption {
  font-size: 0.6rem;
  font-weight: lighter;
  color: var(--sklearn-color-text-muted);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 0.5em;
  text-align: center;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow"><div><div>LogisticRegression</div></div><div><a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></div></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.96875
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.9583333333333334
</pre></div>
</div>
</div>
</div>
<p>Logisitic regression learns a coefficient associated with each feature and each class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([[ 0.29324459,  0.7588186 ],
       [ 1.17054987, -0.31908384],
       [-0.3298721 , -0.84698489],
       [-1.13392236,  0.40725012]])
</pre></div>
</div>
</div>
</div>
<p>For each class there is an intercept.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">intercept_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-0.64417243,  5.10584063,  1.09706504, -5.55873324])
</pre></div>
</div>
</div>
</div>
</section>
<section id="predictions">
<h3>Predictions<a class="headerlink" href="#predictions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Predictions are made by</p>
<ul>
<li><p>getting raw scores for each class</p></li>
<li><p>applying <strong>softmax</strong> instead of <strong>sigmoid</strong> to get probability distribution over a number of classes</p></li>
<li><p>picking the class with the highest prediction probability</p></li>
</ul>
</li>
</ul>
</section>
<section id="sigmoid-vs-softmax">
<h3>Sigmoid vs. Softmax<a class="headerlink" href="#sigmoid-vs-softmax" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>For binary classification, we used the sigmoid function, which “squashes” the raw model output from any number to the range <span class="math notranslate nohighlight">\([0,1]\)</span> using the following formula, where <span class="math notranslate nohighlight">\(x\)</span> is the raw model output.
$<span class="math notranslate nohighlight">\(\frac{1}{1+e^{-x}}\)</span>$</p></li>
<li><p>For multiclass classification, instead of sigmoid, we use softmax, which normalizes a set of raw scores into probabilities.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sigma(\vec{z})_i=\frac{e^{z_i}}{\sum_{j=1}^{K}e^{z_j}}\]</div>
<ul class="simple">
<li><p>It basically makes sure all the outputs are probabilities between 0 and 1, and that they all sum to 1.</p></li>
</ul>
<p>We can examine class probabilities by calling <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> on an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([4.10793260e-03, 2.31298590e-08, 5.83848731e-06, 9.95886206e-01])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">classes_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The prediction here is class 3 because it has the highest predict proba score of 0.995</span>
<span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
</section>
</section>
<section id="introduction-to-neural-networks">
<h2>Introduction to neural networks<a class="headerlink" href="#introduction-to-neural-networks" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Very popular these days under the name <strong>deep learning</strong>.</p></li>
<li><p>Remember this picture we saw at the beginning of the course? Deep learning is a subset of machine learning.</p></li>
</ul>
<p><img alt="" src="../../_images/ai-ml-dl.png" /></p>
<ul class="simple">
<li><p>Neural networks apply a sequence of transformations to the input data.</p></li>
<li><p>You can think of them as a generalization of linear models, where multiple layers of transformations are applied in succession.</p></li>
<li><p>Here is graphical representation of a logistic regression model.</p>
<ul>
<li><p>We have 4 features: <code class="docutils literal notranslate"><span class="pre">x[0],</span> <span class="pre">x[1],</span> <span class="pre">x[2],</span> <span class="pre">x[3]</span></code></p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">mglearn</span>

<span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_logistic_regression_graph</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9802df43f891a430b9abaaf3b966adcc262fb4da8c31530d686c616c82946972.svg" src="../../_images/9802df43f891a430b9abaaf3b966adcc262fb4da8c31530d686c616c82946972.svg" />
</div>
</div>
<ul class="simple">
<li><p>Below we are adding one “layer” of transformations in between features and the target.</p></li>
<li><p>We are repeating the the process of computing the weighted sum multiple times.</p></li>
<li><p>The <strong>hidden units</strong> (e.g., h[1], h[2], …) represent the intermediate processing steps.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_single_hidden_layer_graph</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/3790569389d37d9ea74e2a4269c366b2edbae1df0aeeff440fb7e37faaeccec5.svg" src="../../_images/3790569389d37d9ea74e2a4269c366b2edbae1df0aeeff440fb7e37faaeccec5.svg" />
</div>
</div>
<ul class="simple">
<li><p>Now we are adding one more layer of transformations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mglearn</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">plot_two_hidden_layer_graph</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/916c5dff417ae52fd8b977847bad9cd06126441e9b35c27d975fd507278bfdc6.svg" src="../../_images/916c5dff417ae52fd8b977847bad9cd06126441e9b35c27d975fd507278bfdc6.svg" />
</div>
</div>
<ul class="simple">
<li><p>At a very high level you can also think of them as <code class="docutils literal notranslate"><span class="pre">Pipelines</span></code> in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p></li>
<li><p>A neural network is a model that’s sort of like its own pipeline</p>
<ul>
<li><p>It involves a series of transformations (“layers”) internally.</p></li>
<li><p>The output is the prediction.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/pipeline.png" /></p>
<!-- <img src='./img/pipeline.png' width="800"> -->
<p><a class="reference external" href="https://amueller.github.io/COMS4995-s20/slides/aml-04-preprocessing/#18">Source</a></p>
<ul class="simple">
<li><p>Important question: how many features before/after transformation.</p>
<ul>
<li><p>e.g. scaling doesn’t change the number of features</p></li>
<li><p>OHE increases the number of features</p></li>
</ul>
</li>
<li><p>With a neural net, you specify the number of features after each transformation.</p>
<ul>
<li><p>In the above, it goes from 4 to 3 to 3 to 1.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>To make them really powerful compared to the linear models, we apply a non-linear function to the weighted sum for each hidden node.</p></li>
</ul>
<section id="neural-networks-example-and-terminology">
<h3>Neural networks example and terminology<a class="headerlink" href="#neural-networks-example-and-terminology" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Neural network = neural net</p></li>
<li><p>Deep learning ~ using neural networks</p></li>
<li><p>This type of neural network is also referred to as a <strong>feed-forward neural network</strong></p></li>
</ul>
<p><img alt="" src="../../_images/nn-10.png" /></p>
</section>
<section id="optional-how-does-training-work-in-neural-networks-high-level">
<h3>(Optional) How does training work in neural networks? (High-Level)<a class="headerlink" href="#optional-how-does-training-work-in-neural-networks-high-level" title="Link to this heading">#</a></h3>
<p>Training a neural network has <strong>two main steps:</strong></p>
<ul class="simple">
<li><p><strong>Forward pass:</strong><br />
Feed the input through the network to calculate the predicted output using the current parameter values (weights).</p></li>
<li><p><strong>Backward pass:</strong></p>
<ul>
<li><p>Measure how far the predicted output is from the actual target (this is the <strong>loss</strong>).</p></li>
<li><p>Calculate how to adjust each parameter to improve future predictions (<strong>gradients</strong>).</p></li>
<li><p>Update the parameters using these gradients — this helps the model improve.</p></li>
</ul>
</li>
<li><p>Summary</p>
<ul>
<li><p><strong>Input <span class="math notranslate nohighlight">\(\rightarrow\)</span> Forward Pass <span class="math notranslate nohighlight">\(\rightarrow\)</span> Loss <span class="math notranslate nohighlight">\(\rightarrow\)</span> Backward Pass <span class="math notranslate nohighlight">\(\rightarrow\)</span> Parameter Update <span class="math notranslate nohighlight">\(\rightarrow\)</span> Repeat</strong></p></li>
<li><p>The model repeats this process many times, gradually improving its predictions.</p></li>
</ul>
</li>
</ul>
</section>
<section id="why-neural-networks">
<h3>Why neural networks?<a class="headerlink" href="#why-neural-networks" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>They can learn very complex functions.</p>
<ul>
<li><p>The fundamental tradeoff is primarily controlled by the <strong>number of layers</strong> and <strong>layer sizes</strong>.</p></li>
<li><p>More layers / bigger layers –&gt; more complex model.</p></li>
<li><p>You can generally get a model that will not underfit.</p></li>
</ul>
</li>
<li><p>The work really well for structured data:</p>
<ul>
<li><p>1D sequence, e.g. timeseries, language</p></li>
<li><p>2D image</p></li>
<li><p>3D image or video</p></li>
</ul>
</li>
<li><p>They’ve had some incredible successes in the last 10 years.</p></li>
<li><p>Transfer learning (coming later today) is really useful.</p></li>
</ul>
</section>
<section id="why-not-neural-networks">
<h3>Why not neural networks?<a class="headerlink" href="#why-not-neural-networks" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Often they require a lot of data.</p></li>
<li><p>They require a lot of compute time, and, to be faster, specialized hardware called <a class="reference external" href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPUs</a>.</p></li>
<li><p>They have huge numbers of hyperparameters and are a huge pain to tune.</p>
<ul>
<li><p>Think of each layer having hyperparameters, plus some overall hyperparameters.</p></li>
<li><p>Being slow compounds this problem.</p></li>
</ul>
</li>
<li><p>They are not interpretable.</p></li>
<li><p>When you call <code class="docutils literal notranslate"><span class="pre">fit</span></code>, you are not guaranteed to get the optimal.</p>
<ul>
<li><p>There are now a bunch of hyperparameters specific to <code class="docutils literal notranslate"><span class="pre">fit</span></code>, rather than the model.</p></li>
<li><p>You never really know if <code class="docutils literal notranslate"><span class="pre">fit</span></code> was successful or not.</p></li>
<li><p>You never really know if you should have run <code class="docutils literal notranslate"><span class="pre">fit</span></code> for longer.</p></li>
</ul>
</li>
<li><p>I don’t recommend training them on your own without further training</p>
<ul>
<li><p>Take CPSC 340 and other courses if you’re interested.</p></li>
<li><p>I’ll show you some ways to use neural networks without calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="deep-learning-software">
<h3>Deep learning software<a class="headerlink" href="#deep-learning-software" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>scikit-learn has <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html">MLPRegressor</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">MLPClassifier</a> but they aren’t very flexible.</p>
<ul>
<li><p>In general you’ll want to leave the scikit-learn ecosystem when using neural networks.</p></li>
<li><p>Fun fact: these classes were contributed to scikit-learn by a UBC graduate student.</p></li>
</ul>
</li>
<li><p>There’s been a lot of deep learning software out there.</p></li>
</ul>
<ul class="simple">
<li><p>The current big players are:</p></li>
</ul>
<ol class="arabic simple">
<li><p><a class="reference external" href="http://pytorch.org">PyTorch</a></p></li>
<li><p><a class="reference external" href="https://www.tensorflow.org">TensorFlow</a></p></li>
</ol>
<ul class="simple">
<li><p>Both are heavily used in industry.</p></li>
<li><p>If interested, see <a class="reference external" href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">comparison of deep learning software</a>.</p></li>
</ul>
</section>
</section>
<section id="break-5-min">
<h2>Break (5 min)<a class="headerlink" href="#break-5-min" title="Link to this heading">#</a></h2>
<p><img alt="" src="../../_images/eva-coffee.png" /></p>
<p><br><br><br><br></p>
</section>
<section id="introduction-to-computer-vision">
<h2>Introduction to computer vision<a class="headerlink" href="#introduction-to-computer-vision" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Computer_vision">Computer vision</a> refers to understanding images/videos, usually using ML/AI.</p></li>
<li><p>Computer vision has many tasks of interest:</p>
<ul>
<li><p>image classification: is this a cat or a dog?</p></li>
<li><p>object localization: where is the cat in this image?</p></li>
<li><p>object detection: What are the various objects in the image?</p></li>
<li><p>instance segmentation: What are the shapes of these various objects in the image?</p></li>
<li><p>and much more…</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/vision-apps.jpeg" /></p>
<p>Source: https://learning.oreilly.com/library/view/python-advanced-guide/9781789957211/</p>
<ul class="simple">
<li><p>In the last decade this field has been dominated by deep learning.</p></li>
<li><p>We will explore <strong>image classification</strong>.</p></li>
</ul>
<section id="convolutional-neural-networks-cnns-high-level">
<h3>Convolutional Neural Networks (CNNs) (high level)<a class="headerlink" href="#convolutional-neural-networks-cnns-high-level" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Neural networks come in different shapes depending on the type of data and the task.</p></li>
<li><p>CNNs are commonly used in <strong>image classification, object detection, and medical imaging.</strong></p></li>
<li><p>What’s the problem if we use the above feedforward architecture to learn patterns from images?</p></li>
</ul>
<p><img alt="" src="../../_images/cnn-1.png" /></p>
<ul class="simple">
<li><p>Fully connected networks are not well-suited for images because:</p>
<ul>
<li><p><strong>Too many parameters:</strong> Every pixel connects to every neuron, causing the number of parameters to explode.</p></li>
<li><p><strong>No notion of spatial structure:</strong> The model treats pixels independently and ignores the 2D arrangement of the image.</p></li>
<li><p><strong>Lack of translation invariance:</strong> A pattern appearing in a different part of the image looks completely different to the model, so it must relearn the same pattern in multiple positions.</p></li>
</ul>
</li>
</ul>
<p><strong>Filters</strong></p>
<ul class="simple">
<li><p><strong>Convolutional Neural Networks (CNNs)</strong> address these issues by using filters that <em>slide</em> over the input to detect local patterns.</p></li>
<li><p>Each filter looks for a specific kind of pattern—similar to how a metal detector only reacts when it finds metal.</p></li>
<li><p>In the example below, the filter is a 2×2 grid that slides over the image:</p>
<ul>
<li><p>When the region under the filter matches the pattern the filter is looking for, it produces a large activation.</p></li>
<li><p>Otherwise, it produces a small activation.</p></li>
</ul>
</li>
</ul>
<p><img alt="" src="../../_images/cnn-4.png" /></p>
<p><img alt="" src="../../_images/cnn-5.gif" /></p>
<ul class="simple">
<li><p>Play around with filters: https://setosa.io/ev/image-kernels/</p></li>
</ul>
<p><strong>(Optional) CNNs big picture</strong></p>
<p><img alt="" src="../../_images/cnn_big_picture.png" /></p>
</section>
<section id="transfer-learning">
<h3>Transfer learning<a class="headerlink" href="#transfer-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In practice, very few people train an entire CNN from scratch because it requires a large dataset, powerful computers, and a huge amount of human effort to train the model.</p></li>
<li><p>Instead, a common practice is to download a pre-trained model and fine tune it for your task.</p></li>
<li><p>This is called <strong>transfer learning</strong>.</p></li>
<li><p>Transfer learning is one of the most common techniques used in the context of computer vision and natural language processing.</p>
<ul>
<li><p>In the last lecture we used pre-trained embeddings to create text representations.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>There are many deep learning architectures out there that have been very successful across a wide range of problem, e.g.: AlexNet, VGG, ResNet, Inception, MobileNet, etc.</p></li>
<li><p>Many of these are trained on famous datasets such as ImageNet (which contains 1.2 million labelled images with 1000 categories)</p></li>
</ul>
<p><strong>ImageNet</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.image-net.org/">ImageNet</a> is an image dataset that became a very popular benchmark in the field ~10 years ago.</p>
<ul>
<li><p>There are 14 million images and 1000 classes.</p></li>
<li><p>Here are some example classes.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/ImageNet">Wikipedia article</a> on ImageNet</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;imagenet_classes.txt&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()]</span>
<span class="n">classes</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">110</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;black swan, Cygnus atratus&#39;,
 &#39;tusker&#39;,
 &#39;echidna, spiny anteater, anteater&#39;,
 &#39;platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus&#39;,
 &#39;wallaby, brush kangaroo&#39;,
 &#39;koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus&#39;,
 &#39;wombat&#39;,
 &#39;jellyfish&#39;,
 &#39;sea anemone, anemone&#39;,
 &#39;brain coral&#39;]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>The idea of transfer learning is instead of developing a machine learning model from scratch, you use these available pre-trained models for your tasks either directly or by fine tuning them.</p></li>
<li><p>There are three common ways to use transfer learning in computer vision</p>
<ol class="arabic simple">
<li><p>Using pre-trained models out-of-the-box</p></li>
<li><p>Using pre-trained models as feature extractor and training your own model with these features</p></li>
<li><p>Starting with weights of pre-trained models and fine-tuning the weights for your task.</p></li>
</ol>
</li>
<li><p>We will explore the first two approaches.</p></li>
</ul>
<p><br><br></p>
</section>
<section id="using-pre-trained-models-out-of-the-box">
<h3>Using pre-trained models out-of-the-box<a class="headerlink" href="#using-pre-trained-models-out-of-the-box" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Remember this example I showed you in the intro video (our very first lecture)?</p></li>
<li><p>We used a pre-trained model vgg16 which is trained on the ImageNet data.</p></li>
<li><p>We preprocess the given image.</p></li>
<li><p>We get prediction from this pre-trained model on a given image along with prediction probabilities.</p></li>
<li><p>For a given image, this model will spit out one of the 1000 classes from ImageNet.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict labels with associated probabilities for unseen images</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;test_images/*.*&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">img</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">classify_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------------------------------------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/b9b68287bf4ae836ee407ea7de9e11e3b5188b10000f475f514d64a40e376a58.png" src="../../_images/b9b68287bf4ae836ee407ea7de9e11e3b5188b10000f475f514d64a40e376a58.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                   Class  Probability score
                                 macaque              0.714
patas, hussar monkey, Erythrocebus patas              0.122
      proboscis monkey, Nasalis larvatus              0.098
                   guenon, guenon monkey              0.017
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/80b0e7a44ddd54f2125abc3eb7633ff3974a768947ae595196dabaf20802b9ae.png" src="../../_images/80b0e7a44ddd54f2125abc3eb7633ff3974a768947ae595196dabaf20802b9ae.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                         Class  Probability score
                     tiger cat              0.353
              tabby, tabby cat              0.207
               lynx, catamount              0.050
Pembroke, Pembroke Welsh corgi              0.046
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/8d54e9647b2d73abaaa9c4b098d65095b4f3ab77334b28af66f730077b08aab0.png" src="../../_images/8d54e9647b2d73abaaa9c4b098d65095b4f3ab77334b28af66f730077b08aab0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                     Class  Probability score
         cheetah, chetah, Acinonyx jubatus              0.983
                  leopard, Panthera pardus              0.012
jaguar, panther, Panthera onca, Felis onca              0.004
       snow leopard, ounce, Panthera uncia              0.001
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/b315ed8fa044c796223d166f1f55eb50675c2f95715b5f8251cf9f6b5dac2c60.png" src="../../_images/b315ed8fa044c796223d166f1f55eb50675c2f95715b5f8251cf9f6b5dac2c60.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                        Class  Probability score
Walker hound, Walker foxhound              0.580
             English foxhound              0.091
                  EntleBucher              0.080
                       beagle              0.065
--------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
<p><br><br><br><br></p>
<ul class="simple">
<li><p>We got these predictions without “doing the ML ourselves”.</p></li>
<li><p>We are using <strong>pre-trained</strong> <code class="docutils literal notranslate"><span class="pre">vgg16</span></code> model which is available in <code class="docutils literal notranslate"><span class="pre">torchvision</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torchvision</span></code> has many such pre-trained models available that have been very successful across a wide range of tasks: AlexNet, VGG, ResNet, Inception, MobileNet, etc.</p></li>
<li><p>Many of these models have been pre-trained on famous datasets like <strong>ImageNet</strong>.</p></li>
<li><p>So if we use them out-of-the-box, they will give us one of the ImageNet classes as classification.</p></li>
</ul>
<p><img alt="" src="../../_images/cnn-ex.png" /></p>
<p>Source: https://cezannec.github.io/Convolutional_Neural_Networks/</p>
<p>Let’s see what labels this pre-trained model gives us for some images which are very different from the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict labels with associated probabilities for unseen images</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;UBC_img/*.*&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">:</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">img</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">classify_image</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------------------------------------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e9709227c09b83c6ca67e85cb7bdd2fdda58e3a1ee17399ed14d40ab1b2f7cba.png" src="../../_images/e9709227c09b83c6ca67e85cb7bdd2fdda58e3a1ee17399ed14d40ab1b2f7cba.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                      Class  Probability score
                                        fig              0.637
                                pomegranate              0.193
grocery store, grocery, food market, market              0.041
                                      crate              0.023
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/e0684bc1bb22d4e696b536ea36112f315d43fd2ab7f269bc077f510f86d9d906.png" src="../../_images/e0684bc1bb22d4e696b536ea36112f315d43fd2ab7f269bc077f510f86d9d906.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              Class  Probability score
            obelisk              0.104
               pole              0.077
bell cote, bell cot              0.057
       sliding door              0.045
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/59807a275eb5b7155d71f956c26358de876861e8fafbe216207a562ab2c2af9c.png" src="../../_images/59807a275eb5b7155d71f956c26358de876861e8fafbe216207a562ab2c2af9c.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     Class  Probability score
    castle              0.056
     altar              0.056
  fountain              0.051
park bench              0.049
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/cf34b8628810d86b1026f296dcd811d43980ceb53566fa6793745896c3d0e64c.png" src="../../_images/cf34b8628810d86b1026f296dcd811d43980ceb53566fa6793745896c3d0e64c.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                    Class  Probability score
                                      lakeside, lakeshore              0.568
worm fence, snake fence, snake-rail fence, Virginia fence              0.180
                                                boathouse              0.101
                           mobile home, manufactured home              0.017
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/eb4ad329d20f7af7fb40389c5fe7fac778d8f6377cb2df9c0c9706610d96d8ff.png" src="../../_images/eb4ad329d20f7af7fb40389c5fe7fac778d8f6377cb2df9c0c9706610d96d8ff.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                      Class  Probability score
                 totem pole              0.987
                       pole              0.011
                    sundial              0.000
pedestal, plinth, footstall              0.000
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/858c1c074e8a628eccd9cde9c70796e6971885152d32054640b0ca1477377d95.png" src="../../_images/858c1c074e8a628eccd9cde9c70796e6971885152d32054640b0ca1477377d95.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                                   Class  Probability score
                                       sandbar, sand bar              0.421
                    seashore, coast, seacoast, sea-coast              0.157
breakwater, groin, groyne, mole, bulwark, seawall, jetty              0.071
                                     lakeside, lakeshore              0.036
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/6973cd1ff730871c2ba9dc7b2e804c1c886584a386a93249938715c1d1aedebd.png" src="../../_images/6973cd1ff730871c2ba9dc7b2e804c1c886584a386a93249938715c1d1aedebd.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>                                               Class  Probability score
                                         toilet seat              0.171
                                          safety pin              0.060
bannister, banister, balustrade, balusters, handrail              0.039
                                              bubble              0.035
--------------------------------------------------------------
</pre></div>
</div>
<img alt="../../_images/d76bd58ceb1f6ce3a40face50bbf0f8cd69ee147ed0e4b16f685343b0cb7170c.png" src="../../_images/d76bd58ceb1f6ce3a40face50bbf0f8cd69ee147ed0e4b16f685343b0cb7170c.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>              Class  Probability score
     patio, terrace              0.213
           fountain              0.164
lakeside, lakeshore              0.097
            sundial              0.088
--------------------------------------------------------------
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>It’s not doing very well here because ImageNet don’t have proper classes for these images.</p></li>
<li><p>Here we are using pre-trained models out-of-the-box.</p></li>
<li><p>Can we use pre-trained models for our own classification problem with our classes?</p></li>
<li><p>Yes!!</p></li>
</ul>
<p><br><br></p>
</section>
</section>
<section id="using-pre-trained-models-as-feature-extractor">
<h2>Using pre-trained models as feature extractor<a class="headerlink" href="#using-pre-trained-models-as-feature-extractor" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Let’s use pre-trained models to extract features.</p></li>
<li><p>We will pass our specific data through a pre-trained network to get a feature vector for each example in the data.</p></li>
<li><p>The feature vector is usually extracted from the last layer, before the classification layer from the pre-trained network.</p></li>
<li><p>You can think of each layer as a transformer applying some transformations on the input received.</p></li>
</ul>
<p><img alt="" src="../../_images/cnn-ex.png" /></p>
<p>Source: https://cezannec.github.io/Convolutional_Neural_Networks/</p>
<ul class="simple">
<li><p>Once we extract these feature vectors for all images in our training data, we can train a machine learning classifier such as logistic regression or random forest.</p></li>
<li><p>This classifier will be trained on our classes using feature representations extracted from the pre-trained models.</p></li>
<li><p>Let’s try this out.</p></li>
<li><p>It’s better to train such models with GPU. Since our dataset is quite small, we won’t have problems running it on a CPU.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Using device: </span><span class="si">{</span><span class="n">device</span><span class="o">.</span><span class="n">type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using device: cpu
</pre></div>
</div>
</div>
</div>
<section id="dataset">
<h3>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Let’s try it out on a tiny subset of <a class="reference external" href="https://www.kaggle.com/datasets/kmader/food41?select=food_c101_n10099_r32x32x1.h5">Food-101 dataset</a></p></li>
<li><p>Usually structured data such as this one doesn’t come in CSV files.</p></li>
<li><p>Also, if you are working on image datasets in the real world, they are going to be huge datasets and you do not want to load the full dataset at once in the memory.</p></li>
<li><p>So usually you work on small batches.</p></li>
<li><p>You are not expected to understand all the code to read this dataset. But it’s available here for your reference: <code class="docutils literal notranslate"><span class="pre">../code/dee_learning_code.py</span></code> for your reference.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_dir</span> <span class="o">=</span> <span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s1">&#39;food-transfer-learning&#39;</span>
<span class="n">image_datasets</span><span class="p">,</span> <span class="n">dataloaders</span> <span class="o">=</span> <span class="n">read_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="n">dataset_sizes</span> <span class="o">=</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">[</span><span class="n">x</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]}</span>
<span class="n">class_names</span> <span class="o">=</span> <span class="n">image_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">classes</span>
<span class="n">inputs</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Sample valid Images&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">),(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e94a11da0f57d7162461268399756e992d7a0bef0d620fecc0ff6edce3194be4.png" src="../../_images/e94a11da0f57d7162461268399756e992d7a0bef0d620fecc0ff6edce3194be4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Classes: </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">classes</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Class count: </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Samples:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">image_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First sample: </span><span class="si">{</span><span class="n">image_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">samples</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Classes: [&#39;beet_salad&#39;, &#39;chocolate_cake&#39;, &#39;edamame&#39;, &#39;french_fries&#39;, &#39;pizza&#39;, &#39;spring_rolls&#39;, &#39;sushi&#39;]
Class count: 40, 38, 40, 40, 43, 41, 41
Samples: 283
First sample: (&#39;C:\\Users\\Giulia\\OneDrive - UBC\\Github\\cpsc330-2025W1\\lectures\\data/food-transfer-learning\\train\\beet_salad\\104294.jpg&#39;, 0)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Now for each image in our dataset, we’ll extract a feature vector from a pre-trained model called densenet121, which is trained on the ImageNet dataset.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">densenet</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">densenet121</span><span class="p">(</span><span class="n">weights</span><span class="o">=</span><span class="s2">&quot;DenseNet121_Weights.IMAGENET1K_V1&quot;</span><span class="p">)</span>
<span class="n">densenet</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>  <span class="c1"># remove that last &quot;classification&quot; layer</span>
<span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">Z_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">get_features</span><span class="p">(</span>
    <span class="n">densenet</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;valid&quot;</span><span class="p">]</span>
<span class="p">)</span> <span class="c1"># get_features is defined in ../code/deep_learning_code.py</span>
</pre></div>
</div>
</div>
</div>
<p>Now we have extracted feature vectors for all examples. What’s the shape of these features?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Z_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([283, 1024])
</pre></div>
</div>
</div>
</div>
<p>The size of each feature vector is 1024 because the size of the last layer in densenet architecture is 1024.</p>
<p><img alt="" src="../../_images/densenet-architecture.png" /></p>
<p>Source: https://towardsdatascience.com/understanding-and-visualizing-densenets-7f688092391a</p>
<p>Let’s examine the feature vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Z_train</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>...</th>
      <th>1014</th>
      <th>1015</th>
      <th>1016</th>
      <th>1017</th>
      <th>1018</th>
      <th>1019</th>
      <th>1020</th>
      <th>1021</th>
      <th>1022</th>
      <th>1023</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.000628</td>
      <td>0.001867</td>
      <td>0.005263</td>
      <td>0.000643</td>
      <td>0.142654</td>
      <td>0.544122</td>
      <td>0.000459</td>
      <td>0.004617</td>
      <td>0.515678</td>
      <td>0.000261</td>
      <td>...</td>
      <td>0.409524</td>
      <td>1.786506</td>
      <td>0.286546</td>
      <td>0.811878</td>
      <td>0.481693</td>
      <td>0.179064</td>
      <td>1.025992</td>
      <td>2.119847</td>
      <td>2.038495</td>
      <td>0.832221</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000108</td>
      <td>0.001419</td>
      <td>0.003320</td>
      <td>0.000497</td>
      <td>0.100592</td>
      <td>0.031338</td>
      <td>0.000039</td>
      <td>0.003024</td>
      <td>0.529212</td>
      <td>0.000396</td>
      <td>...</td>
      <td>0.014607</td>
      <td>0.006724</td>
      <td>0.440418</td>
      <td>0.834183</td>
      <td>0.820324</td>
      <td>1.111770</td>
      <td>0.390105</td>
      <td>1.340162</td>
      <td>0.575756</td>
      <td>0.728587</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.000350</td>
      <td>0.006774</td>
      <td>0.001326</td>
      <td>0.000486</td>
      <td>0.140210</td>
      <td>0.257724</td>
      <td>0.000773</td>
      <td>0.004360</td>
      <td>0.162747</td>
      <td>0.000161</td>
      <td>...</td>
      <td>2.170505</td>
      <td>0.576441</td>
      <td>0.673952</td>
      <td>1.197436</td>
      <td>1.226750</td>
      <td>0.869717</td>
      <td>0.113458</td>
      <td>1.695457</td>
      <td>0.203397</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.000423</td>
      <td>0.005459</td>
      <td>0.001522</td>
      <td>0.000987</td>
      <td>0.103520</td>
      <td>1.944745</td>
      <td>0.000781</td>
      <td>0.004451</td>
      <td>0.091491</td>
      <td>0.000342</td>
      <td>...</td>
      <td>1.202743</td>
      <td>2.475197</td>
      <td>1.089163</td>
      <td>0.033112</td>
      <td>0.153165</td>
      <td>1.799486</td>
      <td>0.530468</td>
      <td>1.092399</td>
      <td>0.120932</td>
      <td>4.028654</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.000289</td>
      <td>0.001180</td>
      <td>0.002293</td>
      <td>0.001370</td>
      <td>0.036232</td>
      <td>0.290215</td>
      <td>0.000142</td>
      <td>0.004044</td>
      <td>0.559905</td>
      <td>0.000636</td>
      <td>...</td>
      <td>0.660834</td>
      <td>0.000000</td>
      <td>2.927814</td>
      <td>0.949736</td>
      <td>0.440601</td>
      <td>0.031605</td>
      <td>0.000000</td>
      <td>0.993692</td>
      <td>0.131639</td>
      <td>0.731349</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 1024 columns</p>
</div></div></div>
</div>
<ul class="simple">
<li><p>The features are hard to interpret but they have some important information about the images which can be useful for classification.</p></li>
<li><p>Let’s try out logistic regression on these extracted features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Z_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.8208955223880597
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>This is great accuracy for so little data (we only have 150 examples) and little effort of all different types of food!!!</p></li>
<li><p>With logistic regression and flattened representation of images we would get much worse accuracy.</p></li>
</ul>
<p><br><br></p>
</section>
</section>
<section id="object-detection">
<h2>Object detection<a class="headerlink" href="#object-detection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Another useful task and tool to know is object detection using YOLO model.</p></li>
<li><p>Let’s identify objects in a sample image using a pretrained model called YOLO8.</p></li>
<li><p>List the objects present in this image.</p></li>
</ul>
<p><strong>Object detection using <a class="reference external" href="https://docs.ultralytics.com/">YOLO</a></strong></p>
<p>Let’s try this out using a pre-trained model. We’ll use the <code class="docutils literal notranslate"><span class="pre">ultralytics</span></code> package for this, which you’ll have to install in the course environment.</p>
<p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">ultralytics</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">ultralytics</span><span class="w"> </span><span class="kn">import</span> <span class="n">YOLO</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">&quot;yolov8n.pt&quot;</span><span class="p">)</span>  <span class="c1"># pretrained YOLOv8n model</span>

<span class="n">yolo_input</span> <span class="o">=</span> <span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;yolo_test/3356700488_183566145b.jpg&quot;</span>
<span class="n">yolo_result</span> <span class="o">=</span> <span class="n">DATA_DIR</span> <span class="o">+</span> <span class="s2">&quot;yolo_result.jpg&quot;</span>

<span class="c1"># Run batched inference on a list of images</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">yolo_input</span><span class="p">)</span>  <span class="c1"># return a list of Results objects</span>
<span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="n">yolo_result</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.image</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mpimg</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Load the images</span>
<span class="n">input_img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">yolo_input</span><span class="p">)</span>
<span class="n">result_img</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">yolo_result</span><span class="p">)</span>

<span class="c1"># Create a figure to display the images side by side</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Display the first image</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">input_img</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>  <span class="c1"># Hide the axes</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Original Image&quot;</span><span class="p">)</span>

<span class="c1"># Display the second image</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">result_img</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">)</span>  <span class="c1"># Hide the axes</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Result Image&quot;</span><span class="p">)</span>

<span class="c1"># Show the images</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>C:\Users\Giulia\anaconda3\envs\cpsc330\Lib\site-packages\ultralytics\utils\torch_utils.py:227: UserWarning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (Triggered internally at C:\bld\libtorch_1741738354177\work\aten\src\ATen\ParallelNative.cpp:228.)
  torch.set_num_threads(NUM_THREADS)  # reset OMP_NUM_THREADS for cpu training
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>image 1/1 C:\Users\Giulia\OneDrive - UBC\Github\cpsc330-2025W1\lectures\data\yolo_test\3356700488_183566145b.jpg: 512x640 4 persons, 2 cars, 1 stop sign, 275.5ms
Speed: 8.2ms preprocess, 275.5ms inference, 15.3ms postprocess per image at shape (1, 3, 512, 640)
</pre></div>
</div>
<img alt="../../_images/d9e89bba43989018eaa3ce5e0ae6562356d7c9dd8581d68fadb719baf482c12e.png" src="../../_images/d9e89bba43989018eaa3ce5e0ae6562356d7c9dd8581d68fadb719baf482c12e.png" />
</div>
</div>
<p><br><br><br><br></p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p><strong>Multi-class classification</strong></p>
<ul class="simple">
<li><p>Multi-class classification refers to classification with &gt;2 classes.</p>
<ul>
<li><p>Most sklearn classifiers work out of the box.</p></li>
<li><p>With <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> the situation with the coefficients is a bit different, we get 1 coefficient per feature per class.</p></li>
</ul>
</li>
</ul>
<p><strong>Neural networks</strong></p>
<ul class="simple">
<li><p>Neural networks are a flexible class of models.</p>
<ul>
<li><p>They can be challenging to train and often require significant computational resources.; a lot more on that in CPSC 340.</p></li>
<li><p>They generally require leaving the sklearn ecosystem to tensorflow or pytorch.</p></li>
<li><p>They are particular powerful for structured input like images, videos, audio, etc.</p></li>
</ul>
</li>
<li><p>The good news is we can use pre-trained neural networks.</p>
<ul>
<li><p>This saves us a huge amount of time/cost/effort/resources.</p></li>
<li><p>We can use these pre-trained networks directly or use them as feature transformers.</p></li>
</ul>
</li>
</ul>
<p><strong>Random cool stuff</strong></p>
<ul class="simple">
<li><p>Check out <a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">3Blue1Brown series on Deep Learning</a>.</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=oGvHtpJMO3M">A nice video</a> which gives a high-level introduction to computer vision.</p></li>
<li><p>Style transfer: given a “content image” and a “style image”, create a new image with the content of one and the style of the other.</p>
<ul>
<li><p>Here is the <a class="reference external" href="https://arxiv.org/pdf/1508.06576.pdf">original paper from 2015</a>, see Figure 2.</p></li>
<li><p>Here are more in <a class="reference external" href="https://arxiv.org/pdf/1601.04589.pdf">this 2016 paper</a>; see, e.g. Figures 1 and 7.</p></li>
<li><p>This has been done for video as well; see <a class="reference external" href="https://www.youtube.com/watch?v=Khuj4ASldmU">this video from 2016</a>.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://cs.stanford.edu/people/karpathy/sfmltalk.pdf">Image captioning</a>: Transfer learning with NLP and vision</p></li>
<li><p>Colourization: see <a class="reference external" href="http://iizuka.cs.tsukuba.ac.jp/projects/colorization/en/">this 2016 project</a>.</p></li>
<li><p>Inceptionism: let the neural network “make things up”</p>
<ul>
<li><p><a class="reference external" href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">2015 article</a></p></li>
<li><p>“Deep dream” <a class="reference external" href="https://www.youtube.com/watch?v=dbQh1I_uvjo">video from 2015</a>.</p></li>
</ul>
</li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "cpsc330"
        },
        kernelOptions: {
            name: "cpsc330",
            path: "./lectures/101-103-Giulia-lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'cpsc330'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#imports-and-lo">Imports and LO</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imports">Imports</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning objectives</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#questions-for-you">❓❓ Questions for you</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise">iClicker Exercise</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-classification">Multi-class classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">iClicker Exercise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions">Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-vs-softmax">Sigmoid vs. Softmax</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-neural-networks">Introduction to neural networks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-example-and-terminology">Neural networks example and terminology</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-how-does-training-work-in-neural-networks-high-level">(Optional) How does training work in neural networks? (High-Level)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-neural-networks">Why neural networks?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-not-neural-networks">Why not neural networks?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-software">Deep learning software</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#break-5-min">Break (5 min)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-computer-vision">Introduction to computer vision</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns-high-level">Convolutional Neural Networks (CNNs) (high level)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#transfer-learning">Transfer learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-models-out-of-the-box">Using pre-trained models out-of-the-box</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-pre-trained-models-as-feature-extractor">Using pre-trained models as feature extractor</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#object-detection">Object detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>